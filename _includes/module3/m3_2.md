# Diagonalisering og andrederivertstesten

I 3_1 så vi kun på tilfellet når hessematrisen var diagonal. La oss se hvordan vi kan klassifisere de kritiske punktene når hessematrisen ikke er en diagonalmatrise.

### Kritiske punkter 2: Når hessematrisen ikke er diagonal

### Eksempel 1

La oss se på funksjonen
$$
f(x,y) = xy - x^2 - y^2 + x + 4y.
$$
For å finne de kritiske punktene regner vi ut de partiellderivere
$$
\frac{\partial f}{\partial x}(x,y) = y - 2x + 1, \quad
\frac{\partial f}{\partial y}(x,y) = x - 2y + 4.
$$

Det viser seg at vi har et kritisk punkt, nemlig punktet hvor $x = 2$ og $y=3$. Dette kan vi bekrefte ved å løse ligningssystemet
$$
\begin{align}
y - 2x + 1 &= 0 \\
x - 2y + 4 &= 0.
\end{align}
$$

For å finne hessematrisen regner vi ut
$$
\frac{\partial^2 f}{\partial x^2} = -2, \quad
\frac{\partial^2 f}{\partial y^2} = -2, \quad
\frac{\partial^2 f}{\partial x\partial y} = 1
$$

Hessematrisen er altså ikke diagonal og lik
$$
H = \begin{pmatrix}
-2 & 1 \\
1 & -2
\end{pmatrix}
$$

Hva gjør vi da? Vi har to muligheter:

1. Diagonalisering av $H$ for å deretter bruke testen vi lærte i 3_1.
2. Andrederivertstesten som vi lærer i dette notatet.

Mulighet 2 er, som vi skal se, mer lettvint, mens mulighet 1 gir bedre forståelse av hva som foregår.

### 1. Diagonalisering

Matrisen $H$ er symmetrisk, og kan dermed diagonaliseres. Det vil si (husk matte 1!) det finnes en basis $P=[\vec{p}_1,\vec{p}_2]$ slik at
$$
H = PDP^{-1},
$$
hvor $D$ er en diagonalmatrise. Basisen vi velger har ingen inflytelse på om et punkt er et lokalt maksimum, lokalt minimum eller sadelpunkt. Derfor kan vi se bort fra $P$, og det kritiske punktet kan karakterises ut fra fortegnet til elementene i $D$ (altså egenverdiene), akkurat som i 3_1.

### Eksempel 1 (fortsettelse)

Husk at diagonalelementene til $D$ er egenverdiene $\lambda_1, \lambda_2$, mens $P=[\vec{p}_1,\vec{p}_2]$ dannes av de tilsvarende egenvektorene. 

I dette tilfelle er (beregnet i kodefeltet under)

$$
D = \begin{pmatrix}
-1 & 0 \\
0 & -3
\end{pmatrix},\quad
P = \frac{1}{\sqrt{2}}\begin{pmatrix}
1 & -1 \\
1 & 1
\end{pmatrix}
$$

Begge egenverdier har negativt fortegn. Derfor har vi et makspunkt.

```{pyodide-python}
import matplotlib.pyplot as plt
plt.close("all")

# Importer numpy for å gjøre numeriske beregninger
import numpy as np

# Hessematrisen
H = np.array([[-2, 1], [1, -2]])

# Beregn egenverdiene og -vektorene
egenverdier, egenvektorer = np.linalg.eig(H)


print(f"Egenverdiene er {egenverdier}\n\nEgenvektorene er\n{egenvektorer}")

plt.show()
```

Vi kan bekrefte at dette stemmer i punktet $(2,3)$ ved å plotte funksjonen i $[1,3]\times[2,4]$.

```{pyodide-python}
import numpy as np
import matplotlib.pyplot as plt
plt.close("all")

# Importerer matplotlib for plotting
import matplotlib.pyplot as plt

# Bestemmer at vi skal plotte i 3d, samt størelsen på plottet
fig, ax = plt.subplots(subplot_kw={'projection': '3d'})

# Lager rutenettet for plotting
x, y = np.meshgrid(np.linspace(1, 3, 100), np.linspace(2, 4, 100))

# Lager funksjonsverdiene
z = x*y - x**2 - y**2 + x + 4*y

# Plotter funksjonen
ax.plot_surface(x, y, z)

plt.show()
```

### 2. Andrederivertstesten

Det finnes en del oppskrifter som lar deg hoppe over å diagonalisere (men
som egentlig utledes fra dette). Disse blir ofte kalt andrederivert-testen.

La $\Delta=|H|$ være determinanten til hessematrisen i det kritiske punktet $(x_0,y_0)$. Da har vi at
1. hvis $\Delta>0$ og $\frac{\partial^2 f}{\partial x^2}(x_0,y_0)< 0$, så har vi et lokalt maksimumspunkt.
2. hvis $\Delta>0$ og $\frac{\partial^2 f}{\partial x^2}(x_0,y_0)>0$, så har vi et lokalt minimumspunkt.
3. hvis $\Delta<0$ så har vi et sadelpunkt.

Om $\Delta=0$ betyr det at en av egenverdiene til $H$ må være null. Dermed ender vi opp i tilfellet hvor vi ikke får noe informasjon.

### Eksempel 1 (fortsettelse)

La oss igjen se på funksjonen 
$$
f(x,y) = xy - x^2 - y^2 + x + 4y.
$$ 
Tar vi determinanten til hessematrisen får vi
$$
|H| = \left|
\begin{pmatrix}
-2 & 1 \\
1 & -2
\end{pmatrix}
\right|
= -2\cdot (-2) - 1\cdot 1 = 3
$$

Siden $\Delta = 3 > 0$, og $\frac{\partial^2 f}{\partial x^2}(2,3)= \frac{\partial^2 f}{\partial y^2}(2,3) = -2 < 0$, har vi et maksimumspunkt.

### Eksempel 2

Vi ser på funksjonen

$$
f(x,y) = \sin(xy).
$$

Vi har

$$
\frac{\partial f}{\partial x}(x,y) = y\cos(xy), \quad
\frac{\partial f}{\partial y}(x,y) = x\cos(xy).
$$
For å finne de kritiske punktene løser vi ligningsettet $y\cos(xy)=0$ og $x\cos(xy)=0$. Da får vi at $f$ har kritiske punkter i $(0,0)$ og når $xy=\frac{\pi}{2}+\pi n$ hvor $n$ er et heltall. 


For å klassifisere de kritiske punktene regner vi ut
$$
\frac{\partial^2 f}{\partial x^2}(x,y) = -y^2 \sin(xy), \quad
\frac{\partial^2 f}{\partial x\partial y}(x,y) = \cos(xy) - xy\cos(xy), \quad
\frac{\partial^2 f}{\partial y^2}(x,y) = -x^2 \sin(xy).
$$

Vi ønsker å undersøke det kritisk punkt ved $(0,0)$. Da er hessematrisen lik

$$
H = \begin{pmatrix}
0 & 1 \\
1 & 0
\end{pmatrix}.
$$

Determinanten $\Delta = 0^2 - 1^2 = -1$. Siden determinanten er negativ har vi et sadelpunkt i origo. Dette bekreftes i bildet under.

```{pyodide-python}
import numpy as np
import matplotlib.pyplot as plt
plt.close("all")

# Bestemmer at vi skal plotte i 3d, samt størelsen på plottet
fig, ax = plt.subplots(subplot_kw={'projection': '3d'})

# Lager rutenettet for plotting
x, y = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))

# Lager funksjonsverdiene
z = np.sin(x*y)

# Lager overflateplottet
ax.plot_surface(x, y, z)

plt.show()
```

### Hvorfor fungerer andrederivertstesten? [Ikke pensum]

Det er på grunn av at determinanten til en diagonaliserbar matrise er lik produktet av egenverdiene. Hvis fortegnet til disse er forskjellige, får vi at $\Delta$ er negativ, ellers blir $\Delta$ positiv. Dette vil si at metode 1 og 2 i dette notatet er ekvivalente.