# Den deriverte som en lineær avbildning

*En del av denne teksten ligger på et litt høyere nivå enn det vi forventer i dette kurset. Hvis du synes det blir for vanskelig så henviser vi til forelesningsnotatene*

Anta at vi har en funksjon $f:\mathbb{R}^m\rightarrow\mathbb{R}$. Som vi lærte i 2_1, er den retningsderiverte

$$
\frac{\partial f}{\partial \vec{n}}(\vec{x})
$$

både avhengig av $\vec{x}$, som beskriver hvor vi er i rommet, og $\vec{n}$, som beskriver retning vi tar den deriverte i.  Så vi kan tenkte på den retningsderiverte som en funksjon av både $\vec{x}$ og $\vec{n}$.

Det viser seg at om vi holder $\vec{x}$ fast og ser på retningseriverte som en funksjon av retning $\vec{n}$ så får vi en lineærtransformasjon. Om vi beskriver 
lineærtransformasjonen på matriseform så får vi det vi kommer til å kalle _gradienten_ i punktet $\vec{x}$. I dette jupyternotatet skal vi vise hvordan:

1. vi kan utrykke den retningsderiverte via de partiellderiverte.
2. tolke den deriverte som en funksjon gitt som $\nabla f (\vec{x}):\mathbb{R}^m\to \mathbb{R}^m$.

### 1. Den deriverte som funksjon av retningen $\vec{n}$

La oss forklare det første punktet mer detaljert. La oss si at vi har en funksjon $f:\mathbb{R}^m\to\mathbb{R}$. Som vi sa i introduksjonen, er $\frac{Df}{D\vec{x}}$ gitt som

$$
\frac{\partial f}{\partial \vec{n}}= \frac{Df}{D\vec{x}}(\vec{n})
$$
en lineær transformasjon i $\vec{n}$.
Det er også vanlig å bruke notasjonen $Df$.
Transformasjonen $Df$ kaller vi den _deriverte_ av funksjonen $f$.
Dessverre finnes det mange andre alternative notasjoner for den deriverte, men den vi har valgt er den vanligste.

Som vi vet fra "matematikk 1", kan vi skrive alle lineærtransformasjoner på matriseform. La oss gjøre dette når $m=2$.
Da er enhetsvektorene gitt som $\vec{e}_x = (1,0)$  og $\vec{e}_y = (0,1)$. Som vi vet fra 2_3, er definisjonen av de partiellderiverte

$$
\frac{\partial f}{\partial \vec{e}_x} = \frac{\partial f}{\partial x}, \quad
\frac{\partial f}{\partial \vec{e}_y} = \frac{\partial f}{\partial y}.
$$

Husk at enhver lineær transformasjon $T:\mathbb{R}^2\rightarrow\mathbb{R}$ tar matriseformen
$$
T = \big[
T(\vec{e}_x), T(\vec{e}_y)
\big].
$$

I praksis betyr det at det er nok å vite de partiellderiverte $\frac{\partial f}{\partial x}$ og $\frac{\partial f}{\partial y}$ for å finne den deriverte i hvilket som helst retning
$\vec{n}$. Vi får nemlig

$$
\frac{Df}{D\vec{x}} = \left( \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right).
$$

Det fungerer for funksjoner for $f:\mathbb{R}^m\to \mathbb{R}$. Om vi går igjennom samme prosedyre på nytt får vi at 

$$
\frac{Df}{D\vec{x}} = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2},
\ldots, \frac{\partial f}{\partial x_m}\right).
$$

### Utregning av retningsderiverte med de partielle deriverte

Analysen over er nøkkelen til å beregne alle retningsderiverte ved de partielle deriverte. Vi har nemlig at for at en funksjon $f:\mathbb{R}^2\to \mathbb{R}$ er den retningsderiverte

$$
\frac{\partial f}{\partial \vec{n}}=\frac{Df}{D\vec{x}}(\vec{n}) =  \left( \frac{\partial
   f}{\partial x}, \frac{\partial f}{\partial y} \right)\cdot \vec{n}.
$$
Om vi ønsker å finne den retningsderiverte i et bestemt punkt $\vec{x}_0$, får vi nå
$$
\frac{\partial f}{\partial \vec{n}}(\vec{x}_0) =  \left( \frac{\partial
   f}{\partial x}(\vec{x}_0), \frac{\partial f}{\partial y}(\vec{x}_0) \right)\cdot \vec{n}.
$$
Vi har også den mer generelle formelen for en funksjon $f:\mathbb{R}^m\to \mathbb{R}$ hvor den retningsderiverte kan skrives som 
$$
\frac{\partial f}{\partial \vec{n}} =  \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2},
\ldots, \frac{\partial f}{\partial x_m}\right) \cdot \vec{n}.
$$
På samme måte får vi nå at den retningsderiverte i et bestemt punkt $\vec{x}_0$ er
$$
\frac{\partial f}{\partial \vec{n}}(\vec{x}_0) =  \left(\frac{\partial f}{\partial x_1}(\vec{x}_0), \frac{\partial f}{\partial x_2}(\vec{x}_0),
\ldots, \frac{\partial f}{\partial x_m}(\vec{x}_0) \right)\cdot \vec{n}.
$$


### Eksempel 1
La oss bruke denne metoden til å finne den retningsderiverte til $f(x,y) = x^2 y$ i retningen $\vec{n} = \frac{1}{\sqrt{5}}(1,-2)$ og punktet $\vec{x}=(-1,1)$.

Om vi først finner de partiellderiverte får vi at
$$
\frac{\partial f}{\partial x} = 2xy,\quad \text{og} \quad
\frac{\partial f}{\partial y} = x^2.
$$
Dermed er 
$$
\left(\frac{\partial f}{\partial x}(-1,1),
\frac{\partial f}{\partial y}(-1,1)\right) =(-2, 1) .
$$


Bruker formelen over får vi at den retningsderiverte er
$$
\frac{\partial f}{\partial \vec{n}}(-1, 1) = 
(-2, 1)\cdot \frac{1}{\sqrt{5}}(1,-2)
= \frac{1}{\sqrt{5}}(-2\cdot 1 + 1\cdot (-2)) = \frac{-4}{\sqrt{5}}.
$$

### 2. Den deriverte som en funksjon av posisjon $\vec{x}$: gradient

Vi har sett at den deriverte $\frac{Df}{D\vec{x}}(\vec{x})$ av en skalar funksjon $f(\vec{x}):\mathbb{R}^m \rightarrow \mathbb{R}$ på et bestemt punkt $\vec{x}$ er en radvektor. 

Avbildning $\vec{x}\mapsto \frac{Df}{d\vec{x}}(\vec{x})$ er da en vektor funksjon $\mathbb{R}^m\rightarrow\mathbb{R}^m$. Den kalles for **gradient** og skrives ofte 

$$
\nabla f(\vec{x}) = \begin{pmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_m}
\end{pmatrix}
$$

**Obs! Det er vanlig i denne sammenheng å skrive den som en kolonnevektor, altså $\nabla f(\vec{x}) = (\frac{Df}{d\vec{x}})^T$. Vi har da**

$$
\frac{\partial f}{\partial \vec{n}} = \frac{Df}{d\vec{x}}(\vec{n}) = \vec{n}^T \nabla f = \vec{n}\cdot \nabla f
$$

**siden $(AB)^T = B^T A^T$ for alle matrise $A,B$. Denne konflikt i notasjon kan være forvirrende, men jeg finner ingen utvei. Vi velger vår måte å gjøre det siden det gjør kjerneregelen lettere å forstå.**

### Eksempel 2

La oss tilbake til $f(x,y)=x^2 y$ fra eksempel 1. Vi beregnet de partielle deriverte

$$
\frac{\partial f}{\partial x} = 2xy, \quad
\frac{\partial f}{\partial y} = x^2
$$

Vi får gradienten ved å sette dem opp i en vektor:

$$
\nabla f(x,y) = \begin{pmatrix}
2xy \\
x^2 
\end{pmatrix}
$$

### Eksempel 3

La $f(x) = -\frac{1}{\sqrt{x^2 + y^2}} = -(x^2 + y^2)^{-\frac{1}{2}}$. Da er

$$
\frac{\partial f}{\partial x} = 2x \cdot -\frac{-1}{2}(x^2 + y^2)^{-\frac{3}{2}}
$$
$$
\frac{\partial f}{\partial y} = 2y \cdot -\frac{-1}{2}(x^2 + y^2)^{-\frac{3}{2}}
$$

Slik at
$$
\nabla f(\vec{x}) = \begin{pmatrix}
\frac{x}{(x^2 + y^2)^{\frac{3}{2}}} \\
\frac{y}{(x^2 + y^2)^{\frac{3}{2}}} 
\end{pmatrix}
=
\frac{\vec{x}}{||\vec{x}||^3}
$$

### Tolkning

Hva er isåfall vektoren $\nabla f(\vec{x})$?

Svaret er at den peker i retningen hvor funksjonen øker fortest. En mer jordnær tolkning er at hvis funksjonen $f(x,y)$ viser høyden over bakken i punktet $\vec{x}$, så viser $\nabla f(\vec{x})$ retningen som er brattest. En (ekstremt dyktig!) klatrer vil da alltid følge gradienten i sin ferd oppover.

Tilbake til funksjonen over. Det viser at $f(x)$ er en (uendelig) dyp symmetrisk "brønn" (se under). For å komme fortest mulig ut, følger vi $\nabla f(\vec{x})$, som peker i retning $\vec{x}$, dvs bort fra origoen. Vi klatrer da i en rett linje (se eksempel 4 under bildene).

Ekstra forklaring: i polarkoordinater hvor $r$ er avstand fra origo og $\theta$ vinkelen, er $f(r,\theta) = -\frac{1}{r}$. Forøvrig har vi
$$
\nabla f(r,\theta) = \frac{1}{r^2} \vec{e}_r,
$$
hvor $\vec{e}_r$ er basis vektoren som peker bort fra origoen. Det er et ekstremt viktig eksempel i fysikk da det forklarer en invers kvadratlov, kjent fra blant annet tyngdekraft (Newtons lov) og elektrolære (Coulombs lov). Her er $f$ *potentialet* og $\nabla f$ kraftfeltet.

```{pyodide-python}
import matplotlib.pyplot as plt
plt.close("all")

# Plotter f fra eksempelet over

import matplotlib.pyplot as plt
import numpy as np


fig, ax = plt.subplots()
ax = fig.add_subplot(111, projection='3d')

# Lager gitter in polarkoordinater
r = np.linspace(0.1, 1.25, 50)
p = np.linspace(0, 2*np.pi, 50)
R, P = np.meshgrid(r, p)

# Beregner f
Z = -1/R

# Uttrykker gitteret i vanlige (kartesiske koordinater).
X, Y = R*np.cos(P), R*np.sin(P)

# Plott overflaten.
ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r)

plt.show()
```

```{pyodide-python}
import numpy as np
import matplotlib.pyplot as plt
plt.close("all")

# Plotter nabla f for eksempelet over (strømlinjer)

# forskyver gitteret med 0.25 for å unngå problemet i origo, hvor nabla f ikke er definert
x = 0.25+np.linspace(-5, 5, 21)
y = 0.25+np.linspace(-5, 5, 21)

X, Y = np.meshgrid(x, y)

# U og V er henholdsvis x- og y- komponentene til nabla f(X,Y)
U = X / ((X**2 + Y**2)**(3/2))
V = Y / ((X**2 + Y**2)**(3/2))

# plot
fig, ax = plt.subplots()

ax.quiver(X, Y, U, V)

plt.show()

plt.show()
```

### Eksempel 4: Klatring (Gradient ascent/descent)

Pilene over viser gradienten til funksjonen fra Eksempel 3, altså
$$
\nabla f(\vec{x}) = \begin{pmatrix}
\frac{x}{(x^2 + y^2)^{\frac{3}{2}}} \\
\frac{y}{(x^2 + y^2)^{\frac{3}{2}}} 
\end{pmatrix}
=
\frac{\vec{x}}{||\vec{x}||^3}
$$

Den peker alltid i retning bort fra origo. Lengden på pilene er altså $\frac{1}{r^2}$, hvor $r$ er avstand til origo. Det blir veldig stort når vi nærmere oss origoen, siden brønnen blir brattere og brattere.

La oss anta at klatreren står i punktet $\vec{x}=(3,4)^T$.

Da blir $||(3,4)||=\sqrt{3^2+4^2}=5$, slik at

$$
\nabla f(3,4) = \frac{(3,4)^T}{5^3} = \frac{1}{125}
\begin{pmatrix}
3 \\
4
\end{pmatrix}
$$

En vanlig metode i optimering innebærer å klatre opp/ned en viss avstand i samme retning som gradienten, mer presist å dra fra $\vec{x}$ til

$$
\vec{x} + h\nabla f(\vec{x}).
$$

Hvis vi gjør det med $h=5$ i dette tilfelle havner vi på 

$$
\begin{pmatrix}
3 \\
4
\end{pmatrix}
+ 5 \cdot
\frac{1}{125}
\begin{pmatrix}
3 \\
4
\end{pmatrix}
= \begin{pmatrix}
3.12 \\
4.16
\end{pmatrix}.
$$

Vi kan gjenta steget så mange ganger vi vil. I dette tilfelle bytter vi ikke retning, men for mer kompliserte funksjoner vil retningen typisk endrer seg fra steg til steg.