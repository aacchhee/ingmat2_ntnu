[
  {
    "objectID": "MATPLOTLIB_STYLE_GUIDE.html",
    "href": "MATPLOTLIB_STYLE_GUIDE.html",
    "title": "Matplotlib + Pyodide Style Guide",
    "section": "",
    "text": "This project runs Matplotlib inside Pyodide (browser-based Python). Because all cells share the same interpreter, strict rules apply.\n\n\nThe Matplotlib backend is set once in JavaScript:\nmatplotlib.use(\"module://matplotlib_pyodide.html5_canvas_backend\")\n\n\n\nimport matplotlib.pyplot as plt\nplt.close(\"all\")\n\nfig, ax = plt.subplots(...)\nax.plot(...)\n\nplt.show()\n\n\n\n\n❌ Never call plt.figure() if you also use plt.subplots()\n❌ Never rely on implicit pyplot state\n❌ Never omit plt.close(\"all\")\n❌ Never use plt.show() more than once per cell\n✅ Exactly ONE figure per cell\n\n\n\n\n\n\n\nSymptom\nCause\n\n\n\n\nTwo canvases\nplt.figure() + plt.subplots()\n\n\nOld plot appears\nMissing plt.close(\"all\")\n\n\nFigure(600x400) text\nWrong backend or missing plt.show()\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.close(\"all\")\n\nfig, ax = plt.subplots(figsize=(6,4))\nax.plot(x, y)\n\nplt.show()"
  },
  {
    "objectID": "MATPLOTLIB_STYLE_GUIDE.html#global-setup-handled-automatically",
    "href": "MATPLOTLIB_STYLE_GUIDE.html#global-setup-handled-automatically",
    "title": "Matplotlib + Pyodide Style Guide",
    "section": "",
    "text": "The Matplotlib backend is set once in JavaScript:\nmatplotlib.use(\"module://matplotlib_pyodide.html5_canvas_backend\")"
  },
  {
    "objectID": "MATPLOTLIB_STYLE_GUIDE.html#required-pattern-for-every-plot",
    "href": "MATPLOTLIB_STYLE_GUIDE.html#required-pattern-for-every-plot",
    "title": "Matplotlib + Pyodide Style Guide",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nplt.close(\"all\")\n\nfig, ax = plt.subplots(...)\nax.plot(...)\n\nplt.show()"
  },
  {
    "objectID": "MATPLOTLIB_STYLE_GUIDE.html#rules-do-not-break",
    "href": "MATPLOTLIB_STYLE_GUIDE.html#rules-do-not-break",
    "title": "Matplotlib + Pyodide Style Guide",
    "section": "",
    "text": "❌ Never call plt.figure() if you also use plt.subplots()\n❌ Never rely on implicit pyplot state\n❌ Never omit plt.close(\"all\")\n❌ Never use plt.show() more than once per cell\n✅ Exactly ONE figure per cell"
  },
  {
    "objectID": "MATPLOTLIB_STYLE_GUIDE.html#symptoms-of-violation",
    "href": "MATPLOTLIB_STYLE_GUIDE.html#symptoms-of-violation",
    "title": "Matplotlib + Pyodide Style Guide",
    "section": "",
    "text": "Symptom\nCause\n\n\n\n\nTwo canvases\nplt.figure() + plt.subplots()\n\n\nOld plot appears\nMissing plt.close(\"all\")\n\n\nFigure(600x400) text\nWrong backend or missing plt.show()"
  },
  {
    "objectID": "MATPLOTLIB_STYLE_GUIDE.html#copy-paste-template",
    "href": "MATPLOTLIB_STYLE_GUIDE.html#copy-paste-template",
    "title": "Matplotlib + Pyodide Style Guide",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nplt.close(\"all\")\n\nfig, ax = plt.subplots(figsize=(6,4))\nax.plot(x, y)\n\nplt.show()"
  },
  {
    "objectID": "pages/page6.html",
    "href": "pages/page6.html",
    "title": "01 - Innføring",
    "section": "",
    "text": "Alle lykkelige familier ligner hverandre, hver ulykkelige familie er ulykkelige på sin egen måte (sitat: Leo Tolstoj).\nDen tidligere NTNU-affilierte læreren Ariah Iserles likte å sitere Tolstoj i forbindelse med differensialligninger. Alle ordinære differensialligninger er faktiske like; det finnes alltid et lokalt koordinatbytte som sender en vilkårlig ligning til en annen (lokal på grunn av mulige “eksplosjoner”, hvor løsningen går mot uendelig).\nDet er ikke en nyttig observasjon når det gjelder løsning av ligninger, siden det er vanskeligere å finne et slikt koordinatbytte enn det er å løse ligningen. Men det forklarer hvorfor det finnes flere “En størrelse passer alle”-metoder, slik som Eulers metode, for å løse dem, og lignende teoremer som forsikrer oss at det finnes en entydig løsning som gir mening.\nSlik er det ikke med partielle differensialligninger.\nTeori og praksis på partielle differensialligninger er drevet frem av forskjellige eksempler og det vi vet om dem. Det finnes riktignok en del teorier som gjelder generelt, og spesielt på lineære ligninger, men de er ofte veldig krevende og av mindre interesse for oss.\nVi kommer altså til å se at numeriske metoder må være tilpasset den spesifikke ligningen som løses, i motsetning til Eulers metode som fungerer på alle ordinære differensialligninger.\n\n\nDet enkleste eksemplet vi kan tenke på er kanskje\n\\[\n\\frac{\\partial f}{\\partial x} = 0.\n\\]\nLigningen sier at \\(f\\) ikke forandrer seg i retningen \\(x\\), dvs. \\(f (x_1, y)\\) og \\(f (x_2, y)\\) er de samme for alle verdier \\(x_1\\) og \\(x_2\\). Alle funksjoner av formen \\(f(x, y) = f(y)\\) oppfyller det kravet og er dermed løsninger. Et av mange eksempler er \\(f(x,y)=y^2\\).\nDet ser kanskje forvirrende ut med utrykk som \\(f(x, y) = f(y)\\). Hva har skjedd med \\(x\\)? Husk at \\(f\\) her er en funksjon fra \\(\\mathbb{R}^2\\rightarrow\\mathbb{R }\\). Med \\(f (y)\\) mener vi en funskjon hvor verdiene er uavhengig av \\(x\\). Siden alle slike funksjoner oppfyller ligningen, må vi ha vesentlig mer informasjon, f. eks. randbetingelser, hvis vi ønsker oss en entydig løsning.\nUnder ser vi plottet av funksjonen \\[\nf(x,y) = \\sin(y),\n\\] som er én (av mange) løsninger av ligningen.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nDet enkleste eksemplet med partiellderiverte i begge variable er kanskje\n\\[\n\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial y}.\n\\]\nDenne ligningen er et eksempel på en klasse med partielle differensiallikninger som kalles transportligninger. Vi sjekker ved innsetting at \\(f (x, y) = x^2 + 2 x y + y^2\\) løser denne ligningen, siden både\n\\[\n\\frac{\\partial f}{\\partial x} = 2x + 2y\n\\]\nog\n\\[\n\\frac{\\partial f}{\\partial y} = 2x + 2y.\n\\]\nVi legger merke til at vi kan skrive om ligningen som\n\\[\n\\frac{\\partial f}{\\partial x} - \\frac{\\partial f}{\\partial y} = 0\n\\]\neller med andre ord\n\\[\n\\frac{\\partial f}{\\partial \\vec{n}} = 0, \\quad\n\\vec{n} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix}\n1 \\\\\n-1\n\\end{pmatrix}.\n\\]\nSå ligningen er ikke så annerledes enn eksempel 1. Den sier at en bestemt retningsderivert alltid er null. Vi kunne egentlig byttet koordinater til et system \\(p = x-t, q = x+t\\), og da ville ligningen vært nøyaktig som i eksempel 1. Altså den deriverte mhp. \\(p\\) ville vært null, og funksjonen ville vært en funksjon av \\(q\\) alene. Det viser seg at alle funksjoner på formen\n\\[\nf(x,y) = g(x+y)\n\\]\nløser ligningen. I koden under viser vi et eksempel på dette.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nHvorfor kaller vi ligningen i eksempel 2 for en transportligning?\nLa oss nå anta en av variablene måler tid, vi omdøper \\(y\\) til \\(t\\), og skriver ligningen som\n\\[\n\\frac{\\partial u}{\\partial t} = c\\frac{\\partial u}{\\partial x}, \\quad u(x,0) = g(x)\n\\]\nLøsningen med hensyn på tiden \\(t\\) er altså \\(u(x,t) = g(x+ct)\\)\nTolkningen er da at initialsignalet \\(g(x)\\) flytter seg langs \\(x\\)-aksen med konstant fart \\(c\\).\n\n\n\nPrøv å forstå hvorfor, med bakgrunn i det du kan om grafer av funksjoner av en variabel.\n\n\n\nI stedet for å plotte funksjoner \\(u(x,t)\\) av to variabler (hvor én er tid) som en overflateplott, er det ofte nyttigere å lage en animasjon av \\(u(x,t)\\) som en graf av en variabel \\(x\\), for forskjellige tider \\(t\\). Vi gjør det i koden under:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nMerk at i PDE-litteratur er det vanlig å skrive \\(f_x\\) for den partiellderiverte \\(\\frac{\\partial f}{\\partial x}\\). De andrederiverte blir \\(f_{yx} = \\frac{\\partial^2 f}{\\partial x\\partial y}\\) osv. Det kan virker forvirrende, men det sparer masse plass.\nEn generell partiell differentialligning tar formen\n\\[\nf(u,u_x,u_y,u_{xx},u_{xy},u_{yy},\\ldots) = 0\n\\]\nDen er lineær hvis den tar formen \\[\nf(x,y)u + g(x,y)u_x + h(x,y)u_y + \\ldots = 0\n\\]\nLigningen \\[\nf(x,y)u + g(x,y)u_x + h(x,y)u_y = 0\n\\]\nkalles for førsteordens, siden det kun er vanlige (førsteordens) partiellderiverte \\(u_x\\) og \\(u_y\\) som er med.\nEn ligning \\[\nf(x,y)u + g(x,y)u_x + h(x,y)u_{yy} = 0\n\\]\nderimot er andreordens, siden vi har en andrederivert \\(u_{yy}\\) med.\n\n\n[ikke pensum]\nGenerelt så er det lettere å løse lineære ligninger. De fleste ligningene vi skal se på i dette kurset er enten lineære, eller blant de lettere ikke-lineære ligningene.\n\n\n\nNoen ligninger:\n\n\n\\[\nu_t + c(x,t)u_x = f(x,t)\n\\]\ner lineær og førsteordens. Funksjonene \\(a(x,t)\\) og \\(f(x,t)\\) er ikke et problem for linearitet, da de ikke er avhengig av \\(u\\).\n\n\n\n\\[\ni\\hbar u_t = -\\frac{\\hbar^2}{2m}  u_{xx} + V(x) u = 0\n\\] er lineær og andreordens.\n\n\n\n\\[\nu_{tt} - u_{xx} + u = 0\n\\]\ner lineær og andreordens.\n\n\n\n\\[\nu_{tt} - u_{xx} + \\sin u = 0\n\\]\ner andreordens, men ikke-lineær, siden \\(\\sin u\\) er en ikke-lineær funksjon av \\(u\\).\n\n\n\n\\[\nu_{xxxx} + u_{xxyy} + u_{yyyy} = 0\n\\]\ner lineær og fjerdeordens.\n\n\n\n\\[\nu_t + uu_x + u_{xxx} = 0\n\\]\ner ikke-lineær pga leddet \\(uu_x\\) og tredjeordens.\n\n\n\n\\[\n||\\nabla u|| = 1\n\\]\nsom kan skrives som \\[\nu_x^2 + u_y^2 = 1,\n\\]\ner førsteordens og ikke-lineær, siden \\(u_x\\) og \\(u_y\\) begge opphøyes i andre potens.",
    "crumbs": [
      "PDE",
      "01 - Innføring"
    ]
  },
  {
    "objectID": "pages/page6.html#innføring",
    "href": "pages/page6.html#innføring",
    "title": "01 - Innføring",
    "section": "",
    "text": "Alle lykkelige familier ligner hverandre, hver ulykkelige familie er ulykkelige på sin egen måte (sitat: Leo Tolstoj).\nDen tidligere NTNU-affilierte læreren Ariah Iserles likte å sitere Tolstoj i forbindelse med differensialligninger. Alle ordinære differensialligninger er faktiske like; det finnes alltid et lokalt koordinatbytte som sender en vilkårlig ligning til en annen (lokal på grunn av mulige “eksplosjoner”, hvor løsningen går mot uendelig).\nDet er ikke en nyttig observasjon når det gjelder løsning av ligninger, siden det er vanskeligere å finne et slikt koordinatbytte enn det er å løse ligningen. Men det forklarer hvorfor det finnes flere “En størrelse passer alle”-metoder, slik som Eulers metode, for å løse dem, og lignende teoremer som forsikrer oss at det finnes en entydig løsning som gir mening.\nSlik er det ikke med partielle differensialligninger.\nTeori og praksis på partielle differensialligninger er drevet frem av forskjellige eksempler og det vi vet om dem. Det finnes riktignok en del teorier som gjelder generelt, og spesielt på lineære ligninger, men de er ofte veldig krevende og av mindre interesse for oss.\nVi kommer altså til å se at numeriske metoder må være tilpasset den spesifikke ligningen som løses, i motsetning til Eulers metode som fungerer på alle ordinære differensialligninger.\n\n\nDet enkleste eksemplet vi kan tenke på er kanskje\n\\[\n\\frac{\\partial f}{\\partial x} = 0.\n\\]\nLigningen sier at \\(f\\) ikke forandrer seg i retningen \\(x\\), dvs. \\(f (x_1, y)\\) og \\(f (x_2, y)\\) er de samme for alle verdier \\(x_1\\) og \\(x_2\\). Alle funksjoner av formen \\(f(x, y) = f(y)\\) oppfyller det kravet og er dermed løsninger. Et av mange eksempler er \\(f(x,y)=y^2\\).\nDet ser kanskje forvirrende ut med utrykk som \\(f(x, y) = f(y)\\). Hva har skjedd med \\(x\\)? Husk at \\(f\\) her er en funksjon fra \\(\\mathbb{R}^2\\rightarrow\\mathbb{R }\\). Med \\(f (y)\\) mener vi en funskjon hvor verdiene er uavhengig av \\(x\\). Siden alle slike funksjoner oppfyller ligningen, må vi ha vesentlig mer informasjon, f. eks. randbetingelser, hvis vi ønsker oss en entydig løsning.\nUnder ser vi plottet av funksjonen \\[\nf(x,y) = \\sin(y),\n\\] som er én (av mange) løsninger av ligningen.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nDet enkleste eksemplet med partiellderiverte i begge variable er kanskje\n\\[\n\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial y}.\n\\]\nDenne ligningen er et eksempel på en klasse med partielle differensiallikninger som kalles transportligninger. Vi sjekker ved innsetting at \\(f (x, y) = x^2 + 2 x y + y^2\\) løser denne ligningen, siden både\n\\[\n\\frac{\\partial f}{\\partial x} = 2x + 2y\n\\]\nog\n\\[\n\\frac{\\partial f}{\\partial y} = 2x + 2y.\n\\]\nVi legger merke til at vi kan skrive om ligningen som\n\\[\n\\frac{\\partial f}{\\partial x} - \\frac{\\partial f}{\\partial y} = 0\n\\]\neller med andre ord\n\\[\n\\frac{\\partial f}{\\partial \\vec{n}} = 0, \\quad\n\\vec{n} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix}\n1 \\\\\n-1\n\\end{pmatrix}.\n\\]\nSå ligningen er ikke så annerledes enn eksempel 1. Den sier at en bestemt retningsderivert alltid er null. Vi kunne egentlig byttet koordinater til et system \\(p = x-t, q = x+t\\), og da ville ligningen vært nøyaktig som i eksempel 1. Altså den deriverte mhp. \\(p\\) ville vært null, og funksjonen ville vært en funksjon av \\(q\\) alene. Det viser seg at alle funksjoner på formen\n\\[\nf(x,y) = g(x+y)\n\\]\nløser ligningen. I koden under viser vi et eksempel på dette.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nHvorfor kaller vi ligningen i eksempel 2 for en transportligning?\nLa oss nå anta en av variablene måler tid, vi omdøper \\(y\\) til \\(t\\), og skriver ligningen som\n\\[\n\\frac{\\partial u}{\\partial t} = c\\frac{\\partial u}{\\partial x}, \\quad u(x,0) = g(x)\n\\]\nLøsningen med hensyn på tiden \\(t\\) er altså \\(u(x,t) = g(x+ct)\\)\nTolkningen er da at initialsignalet \\(g(x)\\) flytter seg langs \\(x\\)-aksen med konstant fart \\(c\\).\n\n\n\nPrøv å forstå hvorfor, med bakgrunn i det du kan om grafer av funksjoner av en variabel.\n\n\n\nI stedet for å plotte funksjoner \\(u(x,t)\\) av to variabler (hvor én er tid) som en overflateplott, er det ofte nyttigere å lage en animasjon av \\(u(x,t)\\) som en graf av en variabel \\(x\\), for forskjellige tider \\(t\\). Vi gjør det i koden under:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "PDE",
      "01 - Innføring"
    ]
  },
  {
    "objectID": "pages/page6.html#lineære-og-ikke-lineære-ligninger",
    "href": "pages/page6.html#lineære-og-ikke-lineære-ligninger",
    "title": "01 - Innføring",
    "section": "",
    "text": "Merk at i PDE-litteratur er det vanlig å skrive \\(f_x\\) for den partiellderiverte \\(\\frac{\\partial f}{\\partial x}\\). De andrederiverte blir \\(f_{yx} = \\frac{\\partial^2 f}{\\partial x\\partial y}\\) osv. Det kan virker forvirrende, men det sparer masse plass.\nEn generell partiell differentialligning tar formen\n\\[\nf(u,u_x,u_y,u_{xx},u_{xy},u_{yy},\\ldots) = 0\n\\]\nDen er lineær hvis den tar formen \\[\nf(x,y)u + g(x,y)u_x + h(x,y)u_y + \\ldots = 0\n\\]\nLigningen \\[\nf(x,y)u + g(x,y)u_x + h(x,y)u_y = 0\n\\]\nkalles for førsteordens, siden det kun er vanlige (førsteordens) partiellderiverte \\(u_x\\) og \\(u_y\\) som er med.\nEn ligning \\[\nf(x,y)u + g(x,y)u_x + h(x,y)u_{yy} = 0\n\\]\nderimot er andreordens, siden vi har en andrederivert \\(u_{yy}\\) med.\n\n\n[ikke pensum]\nGenerelt så er det lettere å løse lineære ligninger. De fleste ligningene vi skal se på i dette kurset er enten lineære, eller blant de lettere ikke-lineære ligningene.\n\n\n\nNoen ligninger:\n\n\n\\[\nu_t + c(x,t)u_x = f(x,t)\n\\]\ner lineær og førsteordens. Funksjonene \\(a(x,t)\\) og \\(f(x,t)\\) er ikke et problem for linearitet, da de ikke er avhengig av \\(u\\).\n\n\n\n\\[\ni\\hbar u_t = -\\frac{\\hbar^2}{2m}  u_{xx} + V(x) u = 0\n\\] er lineær og andreordens.\n\n\n\n\\[\nu_{tt} - u_{xx} + u = 0\n\\]\ner lineær og andreordens.\n\n\n\n\\[\nu_{tt} - u_{xx} + \\sin u = 0\n\\]\ner andreordens, men ikke-lineær, siden \\(\\sin u\\) er en ikke-lineær funksjon av \\(u\\).\n\n\n\n\\[\nu_{xxxx} + u_{xxyy} + u_{yyyy} = 0\n\\]\ner lineær og fjerdeordens.\n\n\n\n\\[\nu_t + uu_x + u_{xxx} = 0\n\\]\ner ikke-lineær pga leddet \\(uu_x\\) og tredjeordens.\n\n\n\n\\[\n||\\nabla u|| = 1\n\\]\nsom kan skrives som \\[\nu_x^2 + u_y^2 = 1,\n\\]\ner førsteordens og ikke-lineær, siden \\(u_x\\) og \\(u_y\\) begge opphøyes i andre potens.",
    "crumbs": [
      "PDE",
      "01 - Innføring"
    ]
  },
  {
    "objectID": "pages/page1.html",
    "href": "pages/page1.html",
    "title": "Mini-IDE (Python i nettleseren)",
    "section": "",
    "text": "Denne siden er en mini-IDE for Python som kjører direkte i nettleseren ved hjelp av Pyodide. Her kan du skrive, endre og kjøre kode uten å installere noe lokalt.",
    "crumbs": [
      "IDE"
    ]
  },
  {
    "objectID": "pages/page1.html#velkommen",
    "href": "pages/page1.html#velkommen",
    "title": "Mini-IDE (Python i nettleseren)",
    "section": "",
    "text": "Denne siden er en mini-IDE for Python som kjører direkte i nettleseren ved hjelp av Pyodide. Her kan du skrive, endre og kjøre kode uten å installere noe lokalt.",
    "crumbs": [
      "IDE"
    ]
  },
  {
    "objectID": "pages/page1.html#slik-bruker-du-kodevinduet",
    "href": "pages/page1.html#slik-bruker-du-kodevinduet",
    "title": "Mini-IDE (Python i nettleseren)",
    "section": "Slik bruker du kodevinduet",
    "text": "Slik bruker du kodevinduet\n\nRun Code: kjører koden i vinduet\nAddCodeBlock: legger til et nytt Python-vindu\nRestart: nullstiller Python-miljøet\n\nDu kan fritt endre koden og eksperimentere. Last ned hele sida på nytt dersom noe ser tungt eller ødelagt ut.",
    "crumbs": [
      "IDE"
    ]
  },
  {
    "objectID": "pages/page1.html#eksempel-enkel-python-kode",
    "href": "pages/page1.html#eksempel-enkel-python-kode",
    "title": "Mini-IDE (Python i nettleseren)",
    "section": "Eksempel: enkel Python-kode",
    "text": "Eksempel: enkel Python-kode\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "IDE"
    ]
  },
  {
    "objectID": "pages/page1.html#tips",
    "href": "pages/page1.html#tips",
    "title": "Mini-IDE (Python i nettleseren)",
    "section": "Tips",
    "text": "Tips\n\nDu kan kopiere kode mellom blokker\nFeilmeldinger vises direkte under koden\nFigurer kan lastes ned fra nettleseren\nDin kode blir ikke lagra. Du må kopiere koden et annet sted (VSCode/Editor/o.l.) og så lagre.\nPyodide har et litt anspent forhold til matplotlib. Skal du plotte noe selv, kan du oppleve å få feilmeldinger fra matplotlib. Disse handler i al hovedsak om setup av lerrettet. Ta et fungerende eksempel fra en av modulene og modifiser det slik at det plotter det du trenger.",
    "crumbs": [
      "IDE"
    ]
  },
  {
    "objectID": "pages/page1.html#issues-bugs-forbedringsforslag",
    "href": "pages/page1.html#issues-bugs-forbedringsforslag",
    "title": "Mini-IDE (Python i nettleseren)",
    "section": "Issues? Bugs? Forbedringsforslag?",
    "text": "Issues? Bugs? Forbedringsforslag?\n\nOrdentlig metode\nÅpne ei sak på github https://github.com/aacchhee/ingmat2_ntnu. Lag en god beskrivelse av saka.\n\n\nDen beste metoden\nGå til github https://github.com/aacchhee/ingmat2_ntnu, clone repository (via fork), fix a bug, send pull request.\n\n\nSiste utvei\nDet er aldri for seint å åpne en konto på github. Har du sterke innvendinger mot github, send en epost til andrey.chesnokov@ntnu.no og beskriv saka i eposten.",
    "crumbs": [
      "IDE"
    ]
  },
  {
    "objectID": "pages/page8.html",
    "href": "pages/page8.html",
    "title": "03 - Innføring transportligninger",
    "section": "",
    "text": "I innføringen kalte vi ligningen\n\\[\nu_x = u_y\n\\]\nfor en transportligning. La oss tenke oss at en av variablene er tid. En mer generell versjon av samme ligning er\n\\[\nu_t + a u_x = 0\n\\]\nfor et tall \\(a\\). Dette er ligningen som vi skal studere nå. Senere vil vi se på den enda mer generelle ligningen\n\\[\nu_t + a(x,t,u)u_x = g(x,t),\n\\]\nog den nært beslektede bevaringsloven\n\\[\nu_t + f(x,t,u)_x = g(x,t)\n\\]\n\n\n\n\nSjekk med derivasjon at \\(u(x,t) = g(x-at)\\) løser ligningen \\[\nu_t + a u_x = 0\n\\] for hvilken som helst envariabelfunksjon \\(g\\).\n\n\n\nAnta at vi har en initialbetingelse av formen \\(u(x,0) = f(x)\\).\nSammenlign det med løsningen \\(u(x,t) = g(x-at)\\); vi setter inn \\(t=0\\) og får \\(u(x,0)=g(x)\\). Dvs. vi må sette \\(f=g\\) for at initialbetingelsen skal være tilfredsstilt. Løsningen er altså\n\\[\nu(x,t) = f(x-at)\n\\]\nHva betyr det? Vi ser at løsningen flytter på seg, med hastighet (inkludert retning) bestemt av \\(a\\). Prøv koden under. Prøv også å endre på funksjonen ‘f(x)’ og hastigheten ‘a’.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nDet er også nyttig å se på nivåkurveplot av funksjonen \\(u(x,t)\\) (også kalt kontourplott). Nivåkurvene har et spesielt navn i denne sammenhengen: karakteristikker.\nSiden vi kjenner verdien av løsningen fra initialbetingelsene, så vet vi hvor nivåkurvene skjærer \\(x\\)-aksen. Da kan vi finne løsningen for alle tidspunkt ved å gå langs karakteristikkene.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nEt typisk randverdiproblem er:\n\\[\nu_t + a u_x = 0, \\quad 0&lt;x&lt;1, \\quad u(x,0)=f(x), \\quad  u(0,t)=h(t)\n\\]\nLøsningen er fortsatt \\(u(x,t) = g(x-at)\\), men ved å sette \\(x=0\\) får vi \\(u(0,t) = g(-at)\\), slik at \\(g(-at) = h(t)\\). Dermed ser vi at \\(g(x-at) = g(-at)\\) slik at \\(t\\) må tilfredsstille \\(t = -\\frac{x-at}{a}\\) og vi får\nAlternativ 1\n\\[\nu(x,t) = h\\left( -\\frac{x-at}{a}\\right).\n\\]\nMen hva med den gamle løsningen\nAlternativ 2 \\[\nu(x,t) = f(x-at)?\n\\]\nVi kan forstå svaret bedre med å se på konturene. Det viser seg at løsningen \\(u(x,t)\\) er en av de to alternativene over, avhengig av verdien til \\(x, t\\).\n\n\n\nPrøv å forstå fra plotten under at alternativ 1 inntreffer om \\(x &lt; at\\), mens om \\(x &gt; at\\) får vi alternativ 2.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nVår ‘fasit’ på hva løsningen på \\(u\\) for denne partielle differensialligningen blir metoden:\n\\[\nu^{i+1}_j = \\frac{1}{2}(u^i_{j+1} + u^i_{j-1}) + \\frac{ak}{2h} u^i_{j-1} - \\frac{ak}{2h} u^i_{j+1}\n\\]\nsom heter Lax-Friedrichs metode. Det er ikke nødvendigvis den mest nøyaktige metoden, men den er pålitelig, lite krevende å implementere, og kan lett tilpasses til mer generelle bevaringslover (se senere).\nFormelen leses slik: \\(u^i_j\\) er verdien til funksjonen ved tidssteg nummer \\(i\\) og \\(x\\)-posisjon nummer \\(j\\), dvs. \\(u_j^i = u(x_j,t_i)\\). Når vi begynner så kjenner vi alle funksjonsverdiene ved starttiden, dvs. \\(u^0_j\\) for alle \\(j\\). Formelen forteller oss derfor hvordan vi finner alle funksjonsverdiene ett tidsteg frem i tid.\nI tillegg kommer randbetingelsene, som ikke er tatt med i formelen.\nDe følgende kodecellene må kjøres etter hverandre.\nI den første kodecellen definerer vi Lax–Friedrichs-metoden, som er et numerisk skjema for å løse transportlikningen. Funksjonen tar inn løsningen ved et gitt tidspunkt og beregner løsningen ett tidssteg videre ved å bruke verdier i nabopunktene i rom, samt gitte randbetingelser.\nI den neste kodecellen setter vi opp selve problemet som skal løses. Her definerer vi randbetingelsene på venstre og høyre side, lager et rutenett i rom og tid, og angir initialbetingelsen. Deretter bruker vi Lax–Friedrichs-metoden til å beregne løsningen steg for steg fremover i tid.\nI den siste kodecellen visualiserer vi den numeriske løsningen. Vi lager en animasjon som viser hvordan løsningen \\(u(x,t)\\) utvikler seg over tid, slik at vi kan se hvordan startprofilen transporteres gjennom rommet.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "PDE",
      "03 - Innføring transportligninger"
    ]
  },
  {
    "objectID": "pages/page8.html#en-analytisk-løsning",
    "href": "pages/page8.html#en-analytisk-løsning",
    "title": "03 - Innføring transportligninger",
    "section": "",
    "text": "Sjekk med derivasjon at \\(u(x,t) = g(x-at)\\) løser ligningen \\[\nu_t + a u_x = 0\n\\] for hvilken som helst envariabelfunksjon \\(g\\).\n\n\n\nAnta at vi har en initialbetingelse av formen \\(u(x,0) = f(x)\\).\nSammenlign det med løsningen \\(u(x,t) = g(x-at)\\); vi setter inn \\(t=0\\) og får \\(u(x,0)=g(x)\\). Dvs. vi må sette \\(f=g\\) for at initialbetingelsen skal være tilfredsstilt. Løsningen er altså\n\\[\nu(x,t) = f(x-at)\n\\]\nHva betyr det? Vi ser at løsningen flytter på seg, med hastighet (inkludert retning) bestemt av \\(a\\). Prøv koden under. Prøv også å endre på funksjonen ‘f(x)’ og hastigheten ‘a’.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nDet er også nyttig å se på nivåkurveplot av funksjonen \\(u(x,t)\\) (også kalt kontourplott). Nivåkurvene har et spesielt navn i denne sammenhengen: karakteristikker.\nSiden vi kjenner verdien av løsningen fra initialbetingelsene, så vet vi hvor nivåkurvene skjærer \\(x\\)-aksen. Da kan vi finne løsningen for alle tidspunkt ved å gå langs karakteristikkene.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nEt typisk randverdiproblem er:\n\\[\nu_t + a u_x = 0, \\quad 0&lt;x&lt;1, \\quad u(x,0)=f(x), \\quad  u(0,t)=h(t)\n\\]\nLøsningen er fortsatt \\(u(x,t) = g(x-at)\\), men ved å sette \\(x=0\\) får vi \\(u(0,t) = g(-at)\\), slik at \\(g(-at) = h(t)\\). Dermed ser vi at \\(g(x-at) = g(-at)\\) slik at \\(t\\) må tilfredsstille \\(t = -\\frac{x-at}{a}\\) og vi får\nAlternativ 1\n\\[\nu(x,t) = h\\left( -\\frac{x-at}{a}\\right).\n\\]\nMen hva med den gamle løsningen\nAlternativ 2 \\[\nu(x,t) = f(x-at)?\n\\]\nVi kan forstå svaret bedre med å se på konturene. Det viser seg at løsningen \\(u(x,t)\\) er en av de to alternativene over, avhengig av verdien til \\(x, t\\).\n\n\n\nPrøv å forstå fra plotten under at alternativ 1 inntreffer om \\(x &lt; at\\), mens om \\(x &gt; at\\) får vi alternativ 2.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "PDE",
      "03 - Innføring transportligninger"
    ]
  },
  {
    "objectID": "pages/page8.html#lax-friedrichs-metode",
    "href": "pages/page8.html#lax-friedrichs-metode",
    "title": "03 - Innføring transportligninger",
    "section": "",
    "text": "Vår ‘fasit’ på hva løsningen på \\(u\\) for denne partielle differensialligningen blir metoden:\n\\[\nu^{i+1}_j = \\frac{1}{2}(u^i_{j+1} + u^i_{j-1}) + \\frac{ak}{2h} u^i_{j-1} - \\frac{ak}{2h} u^i_{j+1}\n\\]\nsom heter Lax-Friedrichs metode. Det er ikke nødvendigvis den mest nøyaktige metoden, men den er pålitelig, lite krevende å implementere, og kan lett tilpasses til mer generelle bevaringslover (se senere).\nFormelen leses slik: \\(u^i_j\\) er verdien til funksjonen ved tidssteg nummer \\(i\\) og \\(x\\)-posisjon nummer \\(j\\), dvs. \\(u_j^i = u(x_j,t_i)\\). Når vi begynner så kjenner vi alle funksjonsverdiene ved starttiden, dvs. \\(u^0_j\\) for alle \\(j\\). Formelen forteller oss derfor hvordan vi finner alle funksjonsverdiene ett tidsteg frem i tid.\nI tillegg kommer randbetingelsene, som ikke er tatt med i formelen.\nDe følgende kodecellene må kjøres etter hverandre.\nI den første kodecellen definerer vi Lax–Friedrichs-metoden, som er et numerisk skjema for å løse transportlikningen. Funksjonen tar inn løsningen ved et gitt tidspunkt og beregner løsningen ett tidssteg videre ved å bruke verdier i nabopunktene i rom, samt gitte randbetingelser.\nI den neste kodecellen setter vi opp selve problemet som skal løses. Her definerer vi randbetingelsene på venstre og høyre side, lager et rutenett i rom og tid, og angir initialbetingelsen. Deretter bruker vi Lax–Friedrichs-metoden til å beregne løsningen steg for steg fremover i tid.\nI den siste kodecellen visualiserer vi den numeriske løsningen. Vi lager en animasjon som viser hvordan løsningen \\(u(x,t)\\) utvikler seg over tid, slik at vi kan se hvordan startprofilen transporteres gjennom rommet.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "PDE",
      "03 - Innføring transportligninger"
    ]
  },
  {
    "objectID": "pages/page3.html",
    "href": "pages/page3.html",
    "title": "Del 2 – Derivasjon i flere dimensjoner",
    "section": "",
    "text": "2.1 Derivasjon i flere dimensjoner2.2 Partiellderiverte2.3 Gradient og retningsderiverte2.4 Kjerneregelen2.5 Andrederiverte\n\n\n\nDerivasjon\nVi begynner kalkulus for funksjoner av flere variabler med derivasjon.\n\nRetningsderiverte (innføring)\nDe partiell deriverte\nRetningsderiverte II og gradient\nKjerneregel [ikke pensum]\nDen andre deriverte\n\nLæringsmål:\n\nBeregne de partiell deriverte og forstå deres betydning\nBeregne retningsderiverte og gradient\nBeregne den andre deriverte\n\n\nRetningsderiverte\nLa oss begynne med en funksjon \\(f:\\mathbb{R}^2\\rightarrow\\mathbb{R}.\\)\nVi er vant til å tenke på den deriverte som en vekstrate/stigningstall. Hva vil det i så fall være i to dimensjoner? Står vi på en fjellside, er det klart at hvor bratt terrenget er avhengig av hvilken retning vi går i. Det samme gjelder for andre funksjoner. Så vi må oppgi en retning for å snakke om den deriverte i flere dimensjoner. Om vi skriver dette formelt får vi:\n\\[\n\\frac{\\partial f}{\\partial \\vec{n}}(\\vec{x}) = \\left.\\frac{d}{dt}f(\\vec{x} + t\\vec{n})\\right|_{t=0} =  \\lim_{t\\rightarrow 0} \\frac{f(\\vec{x}+t\\vec{n}) - f(\\vec{x})}{t}.\n\\]\nVi sier at \\(\\frac{\\partial f}{\\partial \\vec{n}}(\\vec{x})\\) er den retningsderiverte til \\(f\\) i punktet \\(\\vec{x}\\) i retningen\\(\\vec{n}\\).\nDet er vanlig å bruke enhetsvektorer når vi snakker om en retning. Alle vektorer (bortsett fra nullvektor) har derimot en retning, og vi finner denne ved å dele vektoren på lengden.\nDet er også vanlig å skrive \\(\\operatorname{D}_{\\vec{n}}(f)\\) for den retningsderiverte. Det er dette som er skrivemåten du stort sett vil se i forelesningene og i oppgavene.\n\n\nEksempel\nLa oss se hvordan det fungerer for funksjonen \\(f(x,y)=\\sin(xy)\\).\nLa \\(\\vec{n}\\) være retningen \\(\\vec{n} = \\frac{1}{\\sqrt{2}}(1,-1)\\) og la \\(\\vec{x}\\) være punktet \\(\\vec{x} = (2,-1)\\). Da er \\[\n\\vec{x} + t\\vec{n} = \\begin{pmatrix}\n2 + \\frac{1}{\\sqrt{2}}t \\\\\n-1  -\\frac{1}{\\sqrt{2}}t\n\\end{pmatrix},\n\\]\nog\n\\[\nf(\\vec{x}+t\\vec{n}) = \\sin\\left(\\left(2 + \\frac{1}{\\sqrt{2}}t \\right)\\left(-1  -\\frac{1}{\\sqrt{2}}t\\right)\\right) = \\sin\\left(-2-\\frac{3}{\\sqrt{2}}t-t^2/2\\right).\n\\]\nSiden \\(f(\\vec{x}+t\\vec{n})\\) er en vanlig envariabelfunksjon som vi kjenner fra matematikk 1, så kan vi beregne den deriverte til \\(f(\\vec{x}+t\\vec{n})\\) med hensyn på \\(t\\) og ved å bruke kjerneregelen. Vi får da\n\\[\n\\frac{df}{dt} = (-t-3/\\sqrt{2})\\cos\\left(-2-\\frac{3}{\\sqrt{2}}t-t^2/2\\right).\n\\]\nSetter vi inn for \\(t=0\\) får vi at den retningsderiverte er\n\\[\n\\left.\\frac{d}{dt}f(\\vec{x} + t\\vec{n})\\right|_{t=0} = -\\frac{3}{\\sqrt{2}}\\cos\\left(-2\\right)=-\\frac{3}{\\sqrt{2}}\\cos\\left(2\\right)\\approx 0.8828.\n\\]\nVi viser funksjonene \\(f(x,y)\\) og \\(f(\\vec{x}+t\\vec{n})\\) i koden under:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSom vi ser fra grafen, så er stigningstallet i punktet \\(t=0\\) omtrent lik $ .f( + t)|_{t=0} . $\nOm vi nå ønsker finne den retningsderiverte i en annen retning eller i et annet punkt, så må vi gå igjennom hele denne prossesen på nytt. Heldigvis finnes det en lettere måte å regne ut den retningsderiverte på enn metoden vi viste over. Dette kommer vi til å lære om i seksjon 2_3.\n\n\nOppgave:\n\nFinn den retningsderiverte til funksjonen \\(f\\) i punktet \\(\\vec{x}=(0,1)\\) i retningen \\(\\vec{n}=(1,0)\\).\nModifiser koden over til å plotte funksjonen langs denne retningen. Stemmer svarene dine overens?\nI vårt eksempel har vi tatt \\(\\vec{n}\\) til å være en enhetsvektor, dvs \\(||\\vec{n}||=1\\). Hva skjer hvis vi velger en vektor med en annen størrelse? For eksempel om \\(\\vec{n}=(3,0)\\)?\n\n\n\n\n\n\nDe partiellderiverte\nDet viser seg at det er lettest å jobbe med retningsderiverte langs koordinataksene. Disse heter de partiellderiverte. Vi forkuserer på beregninger og tolkninger av de partiellederiverte. I arket 2_3 - Gradient og retningsderverte, skal vi bruke de partiellderiverte til å finne andre generelle retningsderiverte på en enkel måte.\n\nDefinisjon og grafisk tolkning\nLa \\(f:\\mathbb{R}^2\\to \\mathbb{R}\\) være en funksjon.\nDen partiellderiverte \\(\\frac{\\partial f}{\\partial x} (x, y)\\) defineres som den deriverte av \\(f (x, y)\\) når \\(y\\) betraktes som konstant. Ekvivalent kan vi definere det som den retningsderiverte i punktet \\((x,y)\\) i retningen \\(\\vec{e_1}=(1,0)\\).\nLikedan er \\(\\frac{\\partial f}{\\partial y}(x, y)\\) den deriverte når \\(x\\) betraktes som konstant. Den ekvivalente definisjonen i dette tilfellet er da at \\(\\frac{\\partial f}{\\partial y}(x, y)\\) kan betraktes som den retningsderiverte i punktet \\((x,y)\\) i retningen \\(\\vec{e_2}=(0,1)\\).\nDen geometriske tolkningen av \\(\\frac{\\partial f}{\\partial x} (x, y)\\) er at dette er stigningstallet i et tverrsnitt som går langs \\(x\\)-aksen. Samme gjelder for \\(\\frac{\\partial f}{\\partial y} (x, y)\\), bare da langs \\(y\\)-aksen. La oss gjøre det hele mer konkret ved å se på noen eksempler.\n\n\nEksempler\n\n\nEksempel 1\nVi ønsker å finne de partiellderiverte til funksjonen \\(f:\\mathbb{R}^2\\to \\mathbb{R}\\) definert som \\(f (x, y) = x y\\). Som nevnt over, kan vi beregne de partiellderiverte akkurat som den ordinære deriverte, da vi holder de variablene som ikke skal deriveres konstant.\nDermed er de partiellderiverte gitt ved \\[\\frac{\\partial f}{\\partial x} (x, y)=\\frac{\\partial (xy)}{\\partial x}=y\\frac{\\partial x}{\\partial x}   = y\\] og \\[\\frac{\\partial f}{\\partial y} (x, y)=\\frac{\\partial (xy)}{\\partial y}=x\\frac{\\partial y}{\\partial y}   = x.\\]\n\n\nEksempel 2\nLa oss denne gangen ta for oss de partiellderiverte til funksjonen \\(g (x, y) = x^3 y^2\\).\nDa er de partiellderiverte gitt ved \\[\\frac{\\partial g}{\\partial x} (x, y)=\\frac{\\partial (x^3 y^2)}{\\partial x} =y^2\\frac{\\partial x^3 }{\\partial x}  = 3 x^2 y^2\\] og \\[\\frac{\\partial g}{\\partial y} (x, y)=\\frac{\\partial (x^3 y^2)}{\\partial y} =x^3\\frac{\\partial y^2 }{\\partial y}  = 2 x^3 y.\\]\n\n\nEksempel 3\nSiste eksempel vi kommer til å gå igjenom er \\(h(x,y) = \\sin(xy).\\) Vi beregner ved å bruke kjerneregelen at \\[\\frac{\\partial h}{\\partial x}(x,y) =\\frac{\\partial \\sin(xy)}{\\partial x}= y\\cos(xy).\\] En lignende beregning gir oss at \\[\\frac{\\partial h}{\\partial y}(x,y)=\\frac{\\partial \\sin(xy)}{\\partial y} = x\\cos(xy).\\]\n\n\n\n\n\nDen deriverte som en lineær avbildning\nEn del av denne teksten ligger på et litt høyere nivå enn det vi forventer i dette kurset. Hvis du synes det blir for vanskelig så henviser vi til forelesningsnotatene\nAnta at vi har en funksjon \\(f:\\mathbb{R}^m\\rightarrow\\mathbb{R}\\). Som vi lærte i 2_1, er den retningsderiverte\n\\[\n\\frac{\\partial f}{\\partial \\vec{n}}(\\vec{x})\n\\]\nbåde avhengig av \\(\\vec{x}\\), som beskriver hvor vi er i rommet, og \\(\\vec{n}\\), som beskriver retning vi tar den deriverte i. Så vi kan tenkte på den retningsderiverte som en funksjon av både \\(\\vec{x}\\) og \\(\\vec{n}\\).\nDet viser seg at om vi holder \\(\\vec{x}\\) fast og ser på retningseriverte som en funksjon av retning \\(\\vec{n}\\) så får vi en lineærtransformasjon. Om vi beskriver lineærtransformasjonen på matriseform så får vi det vi kommer til å kalle gradienten i punktet \\(\\vec{x}\\). I dette jupyternotatet skal vi vise hvordan:\n\nvi kan utrykke den retningsderiverte via de partiellderiverte.\ntolke den deriverte som en funksjon gitt som \\(\\nabla f (\\vec{x}):\\mathbb{R}^m\\to \\mathbb{R}^m\\).\n\n\n1. Den deriverte som funksjon av retningen \\(\\vec{n}\\)\nLa oss forklare det første punktet mer detaljert. La oss si at vi har en funksjon \\(f:\\mathbb{R}^m\\to\\mathbb{R}\\). Som vi sa i introduksjonen, er \\(\\frac{Df}{D\\vec{x}}\\) gitt som\n\\[\n\\frac{\\partial f}{\\partial \\vec{n}}= \\frac{Df}{D\\vec{x}}(\\vec{n})\n\\] en lineær transformasjon i \\(\\vec{n}\\). Det er også vanlig å bruke notasjonen \\(Df\\). Transformasjonen \\(Df\\) kaller vi den deriverte av funksjonen \\(f\\). Dessverre finnes det mange andre alternative notasjoner for den deriverte, men den vi har valgt er den vanligste.\nSom vi vet fra “matematikk 1”, kan vi skrive alle lineærtransformasjoner på matriseform. La oss gjøre dette når \\(m=2\\). Da er enhetsvektorene gitt som \\(\\vec{e}_x = (1,0)\\) og \\(\\vec{e}_y = (0,1)\\). Som vi vet fra 2_3, er definisjonen av de partiellderiverte\n\\[\n\\frac{\\partial f}{\\partial \\vec{e}_x} = \\frac{\\partial f}{\\partial x}, \\quad\n\\frac{\\partial f}{\\partial \\vec{e}_y} = \\frac{\\partial f}{\\partial y}.\n\\]\nHusk at enhver lineær transformasjon \\(T:\\mathbb{R}^2\\rightarrow\\mathbb{R}\\) tar matriseformen \\[\nT = \\big[\nT(\\vec{e}_x), T(\\vec{e}_y)\n\\big].\n\\]\nI praksis betyr det at det er nok å vite de partiellderiverte \\(\\frac{\\partial f}{\\partial x}\\) og \\(\\frac{\\partial f}{\\partial y}\\) for å finne den deriverte i hvilket som helst retning \\(\\vec{n}\\). Vi får nemlig\n\\[\n\\frac{Df}{D\\vec{x}} = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}\\right).\n\\]\nDet fungerer for funksjoner for \\(f:\\mathbb{R}^m\\to \\mathbb{R}\\). Om vi går igjennom samme prosedyre på nytt får vi at\n\\[\n\\frac{Df}{D\\vec{x}} = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2},\n\\ldots, \\frac{\\partial f}{\\partial x_m}\\right).\n\\]\n\n\nUtregning av retningsderiverte med de partielle deriverte\nAnalysen over er nøkkelen til å beregne alle retningsderiverte ved de partielle deriverte. Vi har nemlig at for at en funksjon \\(f:\\mathbb{R}^2\\to \\mathbb{R}\\) er den retningsderiverte\n\\[\n\\frac{\\partial f}{\\partial \\vec{n}}=\\frac{Df}{D\\vec{x}}(\\vec{n}) =  \\left( \\frac{\\partial\n   f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right)\\cdot \\vec{n}.\n\\] Om vi ønsker å finne den retningsderiverte i et bestemt punkt \\(\\vec{x}_0\\), får vi nå \\[\n\\frac{\\partial f}{\\partial \\vec{n}}(\\vec{x}_0) =  \\left( \\frac{\\partial\n   f}{\\partial x}(\\vec{x}_0), \\frac{\\partial f}{\\partial y}(\\vec{x}_0) \\right)\\cdot \\vec{n}.\n\\] Vi har også den mer generelle formelen for en funksjon \\(f:\\mathbb{R}^m\\to \\mathbb{R}\\) hvor den retningsderiverte kan skrives som \\[\n\\frac{\\partial f}{\\partial \\vec{n}} =  \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2},\n\\ldots, \\frac{\\partial f}{\\partial x_m}\\right) \\cdot \\vec{n}.\n\\] På samme måte får vi nå at den retningsderiverte i et bestemt punkt \\(\\vec{x}_0\\) er \\[\n\\frac{\\partial f}{\\partial \\vec{n}}(\\vec{x}_0) =  \\left(\\frac{\\partial f}{\\partial x_1}(\\vec{x}_0), \\frac{\\partial f}{\\partial x_2}(\\vec{x}_0),\n\\ldots, \\frac{\\partial f}{\\partial x_m}(\\vec{x}_0) \\right)\\cdot \\vec{n}.\n\\]\n\n\nEksempel 1\nLa oss bruke denne metoden til å finne den retningsderiverte til \\(f(x,y) = x^2 y\\) i retningen \\(\\vec{n} = \\frac{1}{\\sqrt{5}}(1,-2)\\) og punktet \\(\\vec{x}=(-1,1)\\).\nOm vi først finner de partiellderiverte får vi at \\[\n\\frac{\\partial f}{\\partial x} = 2xy,\\quad \\text{og} \\quad\n\\frac{\\partial f}{\\partial y} = x^2.\n\\] Dermed er \\[\n\\left(\\frac{\\partial f}{\\partial x}(-1,1),\n\\frac{\\partial f}{\\partial y}(-1,1)\\right) =(-2, 1) .\n\\]\nBruker formelen over får vi at den retningsderiverte er \\[\n\\frac{\\partial f}{\\partial \\vec{n}}(-1, 1) =\n(-2, 1)\\cdot \\frac{1}{\\sqrt{5}}(1,-2)\n= \\frac{1}{\\sqrt{5}}(-2\\cdot 1 + 1\\cdot (-2)) = \\frac{-4}{\\sqrt{5}}.\n\\]\n\n\n2. Den deriverte som en funksjon av posisjon \\(\\vec{x}\\): gradient\nVi har sett at den deriverte \\(\\frac{Df}{D\\vec{x}}(\\vec{x})\\) av en skalar funksjon \\(f(\\vec{x}):\\mathbb{R}^m \\rightarrow \\mathbb{R}\\) på et bestemt punkt \\(\\vec{x}\\) er en radvektor.\nAvbildning \\(\\vec{x}\\mapsto \\frac{Df}{d\\vec{x}}(\\vec{x})\\) er da en vektor funksjon \\(\\mathbb{R}^m\\rightarrow\\mathbb{R}^m\\). Den kalles for gradient og skrives ofte\n\\[\n\\nabla f(\\vec{x}) = \\begin{pmatrix}\n\\frac{\\partial f}{\\partial x_1} \\\\\n\\frac{\\partial f}{\\partial x_2} \\\\\n\\vdots \\\\\n\\frac{\\partial f}{\\partial x_m}\n\\end{pmatrix}\n\\]\nObs! Det er vanlig i denne sammenheng å skrive den som en kolonnevektor, altså \\(\\nabla f(\\vec{x}) = (\\frac{Df}{d\\vec{x}})^T\\). Vi har da\n\\[\n\\frac{\\partial f}{\\partial \\vec{n}} = \\frac{Df}{d\\vec{x}}(\\vec{n}) = \\vec{n}^T \\nabla f = \\vec{n}\\cdot \\nabla f\n\\]\nsiden \\((AB)^T = B^T A^T\\) for alle matrise \\(A,B\\). Denne konflikt i notasjon kan være forvirrende, men jeg finner ingen utvei. Vi velger vår måte å gjøre det siden det gjør kjerneregelen lettere å forstå.\n\n\nEksempel 2\nLa oss tilbake til \\(f(x,y)=x^2 y\\) fra eksempel 1. Vi beregnet de partielle deriverte\n\\[\n\\frac{\\partial f}{\\partial x} = 2xy, \\quad\n\\frac{\\partial f}{\\partial y} = x^2\n\\]\nVi får gradienten ved å sette dem opp i en vektor:\n\\[\n\\nabla f(x,y) = \\begin{pmatrix}\n2xy \\\\\nx^2\n\\end{pmatrix}\n\\]\n\n\nEksempel 3\nLa \\(f(x) = -\\frac{1}{\\sqrt{x^2 + y^2}} = -(x^2 + y^2)^{-\\frac{1}{2}}\\). Da er\n\\[\n\\frac{\\partial f}{\\partial x} = 2x \\cdot -\\frac{-1}{2}(x^2 + y^2)^{-\\frac{3}{2}}\n\\] \\[\n\\frac{\\partial f}{\\partial y} = 2y \\cdot -\\frac{-1}{2}(x^2 + y^2)^{-\\frac{3}{2}}\n\\]\nSlik at \\[\n\\nabla f(\\vec{x}) = \\begin{pmatrix}\n\\frac{x}{(x^2 + y^2)^{\\frac{3}{2}}} \\\\\n\\frac{y}{(x^2 + y^2)^{\\frac{3}{2}}}\n\\end{pmatrix}\n=\n\\frac{\\vec{x}}{||\\vec{x}||^3}\n\\]\n\n\nTolkning\nHva er isåfall vektoren \\(\\nabla f(\\vec{x})\\)?\nSvaret er at den peker i retningen hvor funksjonen øker fortest. En mer jordnær tolkning er at hvis funksjonen \\(f(x,y)\\) viser høyden over bakken i punktet \\(\\vec{x}\\), så viser \\(\\nabla f(\\vec{x})\\) retningen som er brattest. En (ekstremt dyktig!) klatrer vil da alltid følge gradienten i sin ferd oppover.\nTilbake til funksjonen over. Det viser at \\(f(x)\\) er en (uendelig) dyp symmetrisk “brønn” (se under). For å komme fortest mulig ut, følger vi \\(\\nabla f(\\vec{x})\\), som peker i retning \\(\\vec{x}\\), dvs bort fra origoen. Vi klatrer da i en rett linje (se eksempel 4 under bildene).\nEkstra forklaring: i polarkoordinater hvor \\(r\\) er avstand fra origo og \\(\\theta\\) vinkelen, er \\(f(r,\\theta) = -\\frac{1}{r}\\). Forøvrig har vi \\[\n\\nabla f(r,\\theta) = \\frac{1}{r^2} \\vec{e}_r,\n\\] hvor \\(\\vec{e}_r\\) er basis vektoren som peker bort fra origoen. Det er et ekstremt viktig eksempel i fysikk da det forklarer en invers kvadratlov, kjent fra blant annet tyngdekraft (Newtons lov) og elektrolære (Coulombs lov). Her er \\(f\\) potentialet og \\(\\nabla f\\) kraftfeltet.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nEksempel 4: Klatring (Gradient ascent/descent)\nPilene over viser gradienten til funksjonen fra Eksempel 3, altså \\[\n\\nabla f(\\vec{x}) = \\begin{pmatrix}\n\\frac{x}{(x^2 + y^2)^{\\frac{3}{2}}} \\\\\n\\frac{y}{(x^2 + y^2)^{\\frac{3}{2}}}\n\\end{pmatrix}\n=\n\\frac{\\vec{x}}{||\\vec{x}||^3}\n\\]\nDen peker alltid i retning bort fra origo. Lengden på pilene er altså \\(\\frac{1}{r^2}\\), hvor \\(r\\) er avstand til origo. Det blir veldig stort når vi nærmere oss origoen, siden brønnen blir brattere og brattere.\nLa oss anta at klatreren står i punktet \\(\\vec{x}=(3,4)^T\\).\nDa blir \\(||(3,4)||=\\sqrt{3^2+4^2}=5\\), slik at\n\\[\n\\nabla f(3,4) = \\frac{(3,4)^T}{5^3} = \\frac{1}{125}\n\\begin{pmatrix}\n3 \\\\\n4\n\\end{pmatrix}\n\\]\nEn vanlig metode i optimering innebærer å klatre opp/ned en viss avstand i samme retning som gradienten, mer presist å dra fra \\(\\vec{x}\\) til\n\\[\n\\vec{x} + h\\nabla f(\\vec{x}).\n\\]\nHvis vi gjør det med \\(h=5\\) i dette tilfelle havner vi på\n\\[\n\\begin{pmatrix}\n3 \\\\\n4\n\\end{pmatrix}\n+ 5 \\cdot\n\\frac{1}{125}\n\\begin{pmatrix}\n3 \\\\\n4\n\\end{pmatrix}\n= \\begin{pmatrix}\n3.12 \\\\\n4.16\n\\end{pmatrix}.\n\\]\nVi kan gjenta steget så mange ganger vi vil. I dette tilfelle bytter vi ikke retning, men for mer kompliserte funksjoner vil retningen typisk endrer seg fra steg til steg.\n\n\n\n\n\nKjerneregelen\nObs! Dette er ikke pensum (kommer ikke på eksamen).\nFormålet her er å vise at kjerneregelen \\[\n\\frac{df}{dx} = \\frac{df}{dg}\\frac{dg}{dx}\n\\] holder også i flere dimensjoner. I denne sammenheng vil vi tenke på den deriverte \\(\\frac{Df}{Dx}\\) som en lineær avbildning, slik som den forrige notaten. Kort fortalt vil kjerneregelen da ta nøyaktig samme form som over. Vi viser to tilfeller:\n\nDen derivert langs en parametrisk kurve (også kalt den totale deriverte)\nVariablebytte og den deriverte\n\n\nDerivasjon mhp en parameter (total deriverte)\nHvis vi har en funksjon \\[\n\\vec{x} = \\begin{pmatrix}\nx(t) \\\\\ny(t)\n\\end{pmatrix}\n\\]\nOg vil finne ut \\(\\frac{df}{dt}\\), når \\(f(t) = f\\big(x(t),y(t) \\big)\\), blir kjerneregelen multiplikasjon:\n\\[\n\\frac{df}{dt} = \\frac{Df}{D\\vec{x}} \\frac{d\\vec{x}}{dt} =\n\\begin{pmatrix}\n\\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{dx}{dt} \\\\\n\\frac{dy}{dt}\n\\end{pmatrix}\n= \\frac{\\partial f}{\\partial x} \\frac{dx}{dt} + \\frac{\\partial f}{\\partial y}\\frac{dy}{dt}\n\\]\n\n\nEksempel 1\nLa \\(x(t) = \\cos(t), y(t)=\\sin(t)\\), og \\(f(x,y)=2x^2 + y\\).\nDa har vi \\[\n\\frac{\\partial f}{\\partial x} = 4x = 4\\cos(t), \\quad \\frac{\\partial f}{\\partial y} = 1.\n\\]\nI tilleg har vi \\[\n\\frac{dx}{dt} = -\\sin(t), \\quad \\frac{dy}{dt} = \\cos(t).\n\\]\nDa blir \\[\n\\frac{df}{dt} = \\frac{\\partial f}{\\partial x} \\frac{dx}{dt} + \\frac{\\partial f}{\\partial y}\\frac{dy}{dt}\n= 4\\cos(t)\\cdot (-\\sin(t)) + 1\\cdot\\cos(t) = \\cos(t)\\big(1-4\\sin(t)\\big).\n\\]\n\n\nVariabelbytte\nDet kan også gjøres med matrisemultiplikasjon, men her vil en variabelbytte \\(g:\\mathbb{R}^2\\rightarrow\\mathbb{R}^2\\) være en funksjon av type vi fokusere på i matte 3. Dens deriverte blir en matrise:\n\\[\n\\frac{D\\vec{g}}{D\\vec{x}} = \\begin{pmatrix}\n\\frac{\\partial g_1}{\\partial x} & \\frac{g_1}{\\partial y} \\\\\n\\frac{\\partial g_2}{\\partial x} & \\frac{g_2}{\\partial y}\n\\end{pmatrix}\n\\]\nDa får vi\n\\[\n\\frac{D\\vec{f}}{D\\vec{x}} =\n\\frac{D\\vec{f}}{D\\vec{g}}\\frac{D\\vec{g}}{D\\vec{x}}\n=\n\\begin{pmatrix}\n\\frac{\\partial f}{\\partial g_1} &\n\\frac{\\partial f}{\\partial g_2}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{\\partial g_1}{\\partial x} & \\frac{g_1}{\\partial y} \\\\\n\\frac{\\partial g_2}{\\partial x} & \\frac{g_2}{\\partial y}\n\\end{pmatrix}\n\\]\nMatrisen over er ofte kalt Jacobimatrisen.\n\n\nPolarkoordinater\nTransformasjonen fra kartesiske til polarkoordinater tar formen\n\\[x = r\\cos\\theta, \\quad y=r\\sin\\theta\\]\nVi har altså \\[\n\\frac{\\partial x}{\\partial r} = \\cos\\theta, \\quad \\frac{\\partial x}{\\partial \\theta} = -r\\sin\\theta\n\\] \\[\n\\frac{\\partial y}{\\partial r} = \\sin\\theta, \\quad \\frac{\\partial x}{\\partial \\theta} = r\\cos\\theta\n\\] Vi har da \\[\nJ = \\begin{pmatrix}\n\\cos\\theta & -r\\sin\\theta \\\\\n\\sin\\theta & r\\cos\\theta\n\\end{pmatrix}\n\\]\n\n\nEksempel 2\nLa oss se på \\(f(r,\\theta)=\\cos(\\theta)\\). Vi har \\[\n\\frac{\\partial f}{\\partial r} = 0, \\quad \\frac{\\partial f}{\\partial \\theta} = -\\sin\\theta,\n\\] slik at \\[\n\\frac{Df}{D\\vec{x}} = \\begin{pmatrix}\n0 & -\\sin\\theta\n\\end{pmatrix}\n\\begin{pmatrix}\n\\cos\\theta & -r\\sin\\theta \\\\\n\\sin\\theta & r\\cos\\theta\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-\\sin^2(\\theta) & r\\sin\\theta\\cos\\theta\n\\end{pmatrix}\n\\]\n\n\n\n\n\nDen andrederiverte\nHva skjer når vi derivere en funksjon \\(f:\\mathbb{R}^2\\rightarrow\\mathbb{R}\\) to ganger?\nSiden \\(\\frac{\\partial f}{\\partial \\vec{n}} (x, y)\\) er en funksjon \\(\\mathbb{R }^2\\rightarrow\\mathbb{R }\\), kan vi derivere den med hensyn på en annen retning \\(\\vec{m}\\). Vi får da den andrederiverte \\(\\frac{\\partial^2f}{\\partial \\vec{n} \\partial \\vec{m}}\\). Akkurat som før er det nok å kun beregne de partiellderiverte\n\\[\nH =\n\\left(\\begin{array}{cc}\n     \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x\n     \\partial y}\\\\\n     \\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial\n     y^2}\n   \\end{array}\\right)\n\\]\nVi har ikke samlet de i en matrise \\(H\\) (som heter Hessematrisen) uten grunn! Generelt er den andrederiverte lik\n\\[\n\\frac{\\partial^2 f}{\\partial \\vec{n} \\partial \\vec{m}} =\n   \\left(\\begin{array}{cc}\n     n_x & n_y\n   \\end{array}\\right) \\left(\\begin{array}{cc}\n     \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x\n     \\partial y}\\\\\n     \\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial\n     y^2}\n   \\end{array}\\right) \\left(\\begin{array}{c}\n     m_x\\\\\n     m_y\n   \\end{array}\\right)\n\\]\nDet viser seg at (stort sett, se eksempel 2 under) \\(\\frac{\\partial^2f}{\\partial y \\partial x}\\)=\\(\\frac{\\partial^2 f}{\\partial x \\partial y}\\). Det betyr at Hessematrisen er symmetrisk, og at rekkefølgen av retningene ikke har noe å si, altså\n\\[\n\\frac{\\partial^2 f}{\\partial \\vec{n} \\partial \\vec{m}} = \\frac{\\partial^2\n   f}{\\partial \\vec{m} \\partial \\vec{n}}\n\\]\n\nEksempel 1\nLa \\(f(x,y) = \\exp(-x)\\sin{y} + 2x + y^2\\).\nDa er \\[\n\\frac{\\partial f}{\\partial x} = -\\exp(-x)\\sin{y} + 2, \\quad\n\\frac{\\partial f}{\\partial y} = \\exp(-x)\\cos{y} + 2y\n\\]\nSlik at \\[\n\\frac{\\partial^2 f}{\\partial x^2} = \\frac{\\partial}{\\partial x} \\frac{\\partial f}{\\partial x}\n= \\frac{\\partial}{\\partial x} \\big( -\\exp(-x)\\sin{y} + 2 \\big) = \\exp(-x)\\sin{y}\n\\] \\[\n\\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial}{\\partial y} \\frac{\\partial f}{\\partial x}\n= \\frac{\\partial}{\\partial y} \\big( -\\exp(-x)\\sin{y} + 2 \\big) = -\\exp(-x)\\cos{y}\n\\] \\[\n\\frac{\\partial^2 f}{\\partial x\\partial y} = \\frac{\\partial}{\\partial x} \\frac{\\partial f}{\\partial y}\n= \\frac{\\partial}{\\partial x} \\big( \\exp(-x)\\cos{y} + 2y \\big) = -\\exp(-x)\\cos{y}\n\\] \\[\n\\frac{\\partial^2 f}{\\partial y^2} = \\frac{\\partial}{\\partial y} \\frac{\\partial f}{\\partial y}\n= \\frac{\\partial}{\\partial y} \\big( \\exp(-x)\\cos{y} + 2y \\big) = -\\exp(-x)\\sin{y} + 2\n\\]\nLinjene 2 og 3 over er som forventet helt like, så vi trengte egentlig bare å beregne ut en av dem. Vi får altså Hessematrisen\n\\[\nH(x,y) =\n\\begin{pmatrix}\n\\exp(-x)\\sin{y} & -\\exp(-x)\\cos{y} \\\\\n-\\exp(-x)\\cos{y} & -\\exp(-x)\\sin{y} + 2\n\\end{pmatrix}\n\\]\nVi ønsker å finne den andrederiverte \\(\\frac{\\partial^2 f}{\\partial \\vec{m}\\partial \\vec{n}}\\) i origo (\\(x=0,y=0\\)) og i retningene \\(\\vec{m} = \\frac{1}{\\sqrt{2}}(1,-1)\\) og \\(\\vec{n} = \\frac{1}{\\sqrt{2}}(1,1)\\). Vi beregner først\n\\[\nH(0,0) =\n\\begin{pmatrix}\n\\exp(-0)\\sin{0} & -\\exp(-0)\\cos{0} \\\\\n-\\exp(-0)\\cos{0} & -\\exp(-0)\\sin{0} + 2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 & -1 \\\\\n-1 & 2\n\\end{pmatrix}\n\\]\nDerfor er \\[\n\\frac{\\partial^2 f}{\\partial \\vec{m}\\partial \\vec{n}} =\n\\frac{1}{\\sqrt{2}}(1,-1)\n\\begin{pmatrix}\n0 & -1 \\\\\n-1 & 2\n\\end{pmatrix}\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix}\n= \\frac{1}{2}(\n1\\cdot 0 \\cdot 1 + 1\\cdot -1\\cdot 1 + -1\\cdot -1\\cdot 1 + -1\\cdot 2\\cdot 1)\n= -1\n\\]\n\n\nMed flere variabler\nDet fungerer på samme måte for funksjoner med flere enn to variable. For eksempel, for funksjoner med tre variabler får vi Hessematrisen\n\\[\nH =\n\\left(\\begin{array}{ccc}\n     \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x\n     \\partial y} & \\frac{\\partial^2 f}{\\partial x\\partial z}\\\\\n     \\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial\n     y^2} & \\frac{\\partial^2 f}{\\partial y\\partial z} \\\\\n      \\frac{\\partial^2 f}{\\partial z \\partial x} & \\frac{\\partial^2 f}{\\partial z \\partial y} & \\frac{\\partial^2 f}{\\partial z^2}\n   \\end{array}\\right)\n\\]\nMatrisen er symmetrisk, og vi har \\(\\frac{\\partial^2 f}{\\partial x_i \\partial x_j}=\\frac{\\partial^2 f}{\\partial x_j \\partial x_i}\\) for alle \\(i,j\\).\n\n\nEksempel 2\nLa oss se på \\(f(x,y,z)=x^2 yz\\).\nDa er\n\\[\n\\frac{\\partial f}{\\partial x} = 2xyz, \\quad\n\\frac{\\partial f}{\\partial y} = x^2 z, \\quad\n\\frac{\\partial f}{\\partial z} = x^2 y\n\\]\nDa har vi\n\\[\n\\frac{\\partial^2 f}{\\partial x^2} = \\frac{\\partial}{\\partial x} 2xyz = 2yz, \\quad\n\\frac{\\partial^2 f}{\\partial x\\partial y} = \\frac{\\partial}{\\partial x} x^2 z = 2xz, \\quad\n\\frac{\\partial^2 f}{\\partial x\\partial z} = \\frac{\\partial}{\\partial x} x^2 y = 2xy\n\\]\nNeste er\n\\[\n\\frac{\\partial^2 f}{\\partial y\\partial x} = \\frac{\\partial}{\\partial y} 2xyz = 2xz, \\quad\n\\frac{\\partial^2 f}{\\partial y^2} = \\frac{\\partial}{\\partial y} x^2 z = 0, \\quad\n\\frac{\\partial^2 f}{\\partial y\\partial z} = \\frac{\\partial}{\\partial y} x^2 y = x^2\n\\]\nOg siste\n\\[\n\\frac{\\partial^2 f}{\\partial z\\partial x} = \\frac{\\partial}{\\partial z} 2xyz = 2xy, \\quad\n\\frac{\\partial^2 f}{\\partial z\\partial y} = \\frac{\\partial}{\\partial z} x^2 z = x^2, \\quad\n\\frac{\\partial^2 f}{\\partial z\\partial z} = \\frac{\\partial}{\\partial z} x^2 y = 0\n\\]\nDen andrederiverte (Hessematrisen) er da\n\\[\nH =\n\\begin{pmatrix}\n2yz & 2xz & 2xy \\\\\n2xz & 0 & x^2 \\\\\n2xy & x^2 & 0\n\\end{pmatrix}\n\\]\n\n\nNoen kommentarer utenfor pensum.\nDet er mulig å finne på eksempler hvor \\(\\frac{\\partial^2 f}{\\partial y \\partial x}\\neq\\frac{\\partial^2 f}{\\partial x \\partial y}\\). Det kan skje hvis de partielle deriverte ikke er kontinuerlige i nærheten av et punkt. Dette er egentlig ikke eksamenspensum, men eksemplene under kan bidra til økt forståelse.\n\n\nEksempel 3\nKanskje det letteste eksemplet på en slik funksjon er \\[\nf(x,y) =\n\\begin{cases}\n0 & y \\neq 0 \\\\\n1 & y=0\n\\end{cases}\n\\]\nSiden \\(f(x,y)\\) er uavhengig av \\(x\\) er \\(\\frac{\\partial f}{\\partial x} = 0\\). Dermed er \\(\\frac{\\partial^2 f}{\\partial y \\partial x} = 0\\). Men \\(\\frac{\\partial f}{\\partial y}\\) er ikke definert i origo, så \\(\\frac{\\partial^2 f}{\\partial y \\partial x}\\) finnes ikke.\n\n\nEksempel 4\nEn muligens mer opplysende (men litt vanskeligere) eksempel er\n\\[\nf(x,y) =\n\\begin{cases}\n0 & (x,y) = (0,0) \\\\\n\\frac{xy(x^2-y^2)}{x^2 + y^2} & (x,y)\\neq(0,0)\n\\end{cases}\n\\]\nOm vi ser bort fra origo, har vi at de partiellderiverte er \\[\n\\frac{\\partial f}{\\partial x} = \\frac{x^4 y + 4y^2 x^3 - y^5}{(x^2 + y^2)^2}\n\\] \\[\n\\frac{\\partial f}{\\partial y} = \\frac{x^5 - 4x^3 y^2 - xy^4}{(x^2 + y^2)^2}.\n\\]\nFor å forstå hva som skjer i origo trenger vi et par grenseverdier. Det viser seg nemlig at\n\\[\n\\frac{\\partial f}{\\partial x}(0,y) = -y, \\quad\n\\frac{\\partial f}{\\partial y}(x,0) = x\n\\]\nDermed har vi \\[\n\\frac{\\partial^2 f}{\\partial x\\partial y}(0,0) = -1, \\quad\n\\frac{\\partial^2 f}{\\partial y\\partial x}(0,0) = 1\n\\] som ikke er like hverandre.",
    "crumbs": [
      "Fellesmodul",
      "Del 2 – Derivasjon i flere dimensjoner"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matematikk for ingeniørfag 2",
    "section": "",
    "text": "Velg ei side fra venstre kolonne."
  },
  {
    "objectID": "pages/page5.html",
    "href": "pages/page5.html",
    "title": "Del 4 – Taylors teorem",
    "section": "",
    "text": "4.1 Taylors teorem4.2 Restledd4.3 Taylors teorem – flere variabler\n\n\n\nTaylors teorem\nDet finnes et uendelig mangfold av funksjoner der ute! For å gjøre det litt mer håndterlig, er det ofte nyttig å bruke bedre kjente funksjoner som en tilnærming.\nDen aller viktigste slik tilnærminger bruker Taylors teorem, som handler om approksimasjon med polynomer.\n\nFunksjoner av en variabel\nFunksjoner av en variabel II: restledd\nFunksjoner av flere variabler \\(\\mathbb{R}^n\\rightarrow\\mathbb{R}\\)\n\nLæringsmål\n\nForstår funksjonstilnærming av 1d funksjoner med Taylors teorem\nForstår middelverdisetning og uttrykk for restledd\nBruk restleddet til å estimere feilen ved Taylortilnærming\nUndersøk (med datamaskin) hva slags problemer kan oppstå med slike tilnærminger\nUtfør linearisering av skalare funksjoner av to variabler\nUtfør andre ordens Taylortilnærminger av funksjoner av to variabler, og koble mot kritisk punkter og den andre deriverte\n\nVi skal altså finne tilnærminger\n\\[\nf(x) \\approx P_k(x),\n\\]\nhvor \\(P_k(x)\\) er et polynom av orden \\(k\\), dvs. \\(P_k(x) = a_k x^k + a_{k-1} x^{k-1} + \\cdots\\). Vi begynner med \\(k=1\\), som vi kaller for en lineær tilnærming, eller lineærisering. Etterpå tar vi \\(k=2\\), og så ser vi på helt vilkårlig \\(k\\).\n\n1. Lineær tilnærming\nVi kan bruke tangentlinjen gjennom et punkt \\(x_0\\) til å tilnærme \\(f(x)\\), hvor \\(x\\) er i nærheten av \\(x_0\\).\nHusk at tangentlinjen gjennom \\((x_0, f(x_0))\\) er gitt av \\[\ny = f(x_0) + f'(x_0)(x-x_0)\n\\]\naltså blir vår tilnærming \\[\nf(x) \\approx f(x_0) + f'(x_0)(x-x_0).\n\\]\n\n\nEksempel 1: geometrisk optikk\nLa \\(f(x)=\\sin(x)\\) og \\(x_0 = 0\\). Da får vi\n\\[\nf(x_0) = f(0) = \\sin(0) = 0\n\\] og siden \\(f'(x) = \\cos(x)\\) får vi \\[\nf'(x_0) = \\cos(x_0) = 1\n\\]\nVi har derfor \\[\n\\sin(x) \\approx 0 + 1\\cdot x = x\n\\]\nVi har også \\(\\sin(x)\\approx x\\) for små vinkler \\(x\\).\nDenne tilnærmingen er ekstremt viktig i optikk, blant annet. Snells lov (https://snl.no/Snells_lov) forklarer brytingen i en overflate ved\n\\[\nn_1 \\sin(\\theta_1) = n_2 \\sin(\\theta_2),\n\\]\nhvor \\(n_i\\) er brytningsindeksene til materialene, og \\(\\theta_i\\) vinklene mot normalen til inn- og utfallende lys. Det kan vises ut fra Snells lov (prøv selv, det er god repitisjon i videregående trigonometri!) at parallelle linjer møter i det samme punkt om \\(\\sin(\\theta_i)=\\theta_i\\). Avstand til punktet heter fokallengde.\nHvis tilnærmingen bryter ned, kommer ikke punktene sammen likevel. Vi får da sfærisk aberrasjon (https://no.wikipedia.org/wiki/Sfærisk_aberrasjon). Bildet under viser den ideelle mot den reelle.\n\n\n\nSpherical aberration\n\n\n\n\nEksempel 2\nFinn en lineær tilnærming til \\(\\log(x)\\).\nDet er ikke så smart å ta \\(x_0=0\\) siden \\(\\log(0)\\) er ikke definert. La oss heller prøve med \\(x_0=1\\).\nVi har \\(f'(x) = \\frac{1}{x}\\).\nDen linære tilnærmingen av \\(\\log(x)\\) rundt \\(x_0 = 1\\) blir altså\n\\[\n\\log(x) \\approx x-1\n\\]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n2. Andreordens tilnærming\nVi får bedre nøyaktighet hvis vi heller bruker et parabel. Det kan vises at den beste tilnærming av \\(f(x)\\) med et andreordens polynom blir\n\\[\nf(x) \\approx  f(x_0) + f'(x_0) (x-x_0) + \\frac{1}{2}f''(x_0) (x-x_0)^2\n\\]\n\n\nEksempel 3\nLa oss fortsette med \\(\\log(x)\\). Vi har \\(f''(x) = -x^{-2}\\), slik at \\(f''(1)=-1\\). Vi får altså:\n\\[\n\\log(x) \\approx (x-1) - \\frac{1}{2}(x-1)^2\n\\]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n3. Generell tilnærming (orden \\(k\\))\nVi kan fortsette med så mange deriverte som vi vil (forutsatt at de eksisterer). Det viser seg at den beste tilnærming er:\n\\[\n\\begin{align*}\nf(x) \\approx & f(x_0) + f'(x_0) (x-x_0) %+ \\frac{1}{2}f''(x) (x-x_0)^2\n+ \\ldots\n+ \\frac{1}{k!} f^{(k)}(x_0) (x-x_0)^k \\\\\n% &+ \\frac{1}{(n+1)!} f^{(n+1)}(a) (x-x_0)^{n+1}\n\\end{align*}\n\\]\nDen kalles for Taylor utviklingen av orden \\(k\\).\n\n\nEksempel 4\nRegn ut Taylor tilnærming av orden \\(k\\) til \\(f=\\log(x)\\), rundt \\(x_0 = 1\\).\nVi har allerede beregnet \\(f'(x)=x^{-1}\\) og \\(f''(x) = -x^{-2}\\). Den tredje deriverte er da \\(f^{(3)}(x) = 2x^{-3}\\), den fjerde er \\(f^{(4)}(x)=2\\cdot -3x^{-4}\\), og generelt er \\(f^{(k)}(x) = (-1)^{k-1} (k-1)! x^{-k}\\). Vi har\n\\[\n\\log(x) \\approx (x-1) - \\frac{1}{2} (x-1)^2 + \\ldots + (-1)^{k-1} \\frac{(k-1)!}{k!} (x-1)^2\n\\]\nSiden \\(\\frac{(k-1)!}{k!} = \\frac{1}{k}\\) har vi\n\\[\n\\log(x) \\approx (x-1) - \\frac{1}{2} (x-1)^2 + \\ldots + (-1)^{k-1} \\frac{1}{k} (x-1)^2\n\\]\n\n\nEksempel 5\nBruk den tredje ordens Taylor utviklingen rundt \\(x_0=1\\) til å finne an approksimasjon til \\(\\sqrt{1.1}\\).\nVi tar altså \\(f(x)=\\sqrt{x}\\), slik at\n\\[\nf'(x) = \\frac{1}{2}x^{-\\frac{1}{2}}, \\quad\nf''(x) = -\\frac{1}{4}x^{-\\frac{3}{2}}, \\quad\nf^{(3)}(x) = \\frac{3}{8}x^{-\\frac{5}{2}}\n\\]\nVed å sette inn \\(x=1\\) får vi \\[\nf'(1) = \\frac{1}{2}, \\quad\nf''(1) = -\\frac{1}{4}, \\quad\nf^{(3)}(1) = \\frac{3}{8}.\n\\]\nSiden \\(f(1) = \\sqrt{1} = 1\\), får vi\n\\[\n\\begin{align*}\nf(x+1) &\\approx f(1) + \\frac{1}{2}x - \\frac{1}{4}\\frac{1}{2!}x^2 + \\frac{3}{8}\\frac{1}{3!}x^3 \\\\\n&= 1 + \\frac{1}{2}x - \\frac{1}{8} x^2 + \\frac{1}{16}x^3\n\\end{align*}\n\\]\nLa oss bruke det til å finne en tilnærming til \\(\\sqrt{1.1}\\). Vi har:\n\\[\n\\sqrt{1.1}=f(0.1 + 1) \\approx 1 + \\frac{1}{2} 0.1 - \\frac{1}{8} 0.01 + \\frac{1}{16} 0.001 = 1 + 0.05 - 0.00125 + 0.0000625 = 1.0488125\n\\]\nDen ekte verdien er \\(\\sqrt{1.1} = 1.04880884\\ldots\\)\n\n\nEr tilnærmingen bra?\nVi skal undersøke tilnærmingen til logaritmen i øvingen. I den neste seksjonen vil vi vise en presis måte å estimere feilen.\n\n\n\n\n\nRestledd\nHer fokuserer vi på restleddet i Taylors teorem, noe som også gjøre teoremet presist og forklarer hvorfor det fungerer.\nHovedfokuset vårt er på anvendelser med feilestimering.\n\n1. Bakgrunn: Middelverdisetningen\nDen enkleste versjonen av middelverdisteningen sier at det finnes et tall \\(a\\in(x_0,x)\\) slik at\n\\[\nf(x) = f(x_0) + f'(a)(x-x_0)\n\\]\nDette er da den enkleste mulig form av Taylors teorem. Litt forenklet får vi Taylors teorem ved å anvende middelverdisetningen en gang til på \\(f'(x)\\), og så på \\(f''(x)\\) og så videre.\nFor eksempel har vi at \\[\nf'(a) = f'(x_0) + f''(b)(a-x_0)\n\\]\nSetter vi dette inn i forrige uttrykk får vi at\n\\[\n\\begin{align}\nf(x) &= f(x_0) + \\big(f'(x_0) + f''(b)(a-x_0)\\big)(x-x_0) \\\\\n&= f(x_0) + f'(x_0)(x-x_0) + f''(b)(a-x_0)(x-x_0)\n\\end{align}\n\\]\nDet er ikke helt den formuleringen vi ønsker, men for å komme skal vi se på en alternativ versjon av middelverdisetningen.\n\n\nEksempel 1\nMen hva sier egentlig teoremet?\nVi ser på funksjonen\n\\[\nf(x) = x + (x-1)(x-2)(x-3), \\quad 1\\leq x\\leq 3\n\\]\nMiddelverdisetningen sier at det finnes et punkt \\(a\\) mellom 1 og 3, slik at\n\\[\nf(3) = f(1) + f'(a)(3-1),\n\\]\nDet kan vi skrive om til\n\\[\n\\frac{f(3)-f(1)}{3-1} = f'(a),\n\\]\nhvor venstresiden er gjennomsnittlig veksthastighet fra 1 til 3. Setningen sier altså at det finnes minst en verdi \\(a\\) hvor veksthastigheten \\(f'(a)\\) er nøyaktig lik gjennomsnittsveksthastigheten over hele intervallet.\nVi beregner\n\\[\nf'(x) = 1 + (x-1)(x-2) + (x-1)(x-3) + (x-2)(x-3) = 1 + (x^2 - 3x + 2) + (x^2 - 4x + 3) + (x^2 - 5x + 6) = 3x^2 -12x + 12\n\\]\nNå er \\(f(1)=1, f(3)=3\\), slik at\n\\[\n\\frac{f(3)-f(1)}{3-1} = 1.\n\\]\nVi kan løse \\(f'(a)=1\\), og finner to løsninger\n\\[\na = \\frac{12 \\pm \\sqrt{(-12)^2 - 4\\cdot 3\\cdot 11}}{2\\cdot 3} = 2 \\pm \\frac{\\sqrt{12}}{6} = 2 \\pm \\frac{\\sqrt{3}}{3}\n\\]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nCauchys middelverdisetning\n[ikke pensum, men tas med for opplysningens skyld]\nDet finnes et tall \\(a\\in[x_0,x]\\) slik at\n\\[\n\\frac{f(x)-f(x_0)}{g(x)-g(x_0)} = \\frac{f'(a)}{g'(a)}\n\\]\nDette kan brukes til å bevise l’Hopitals regel, en gammel favoritt fra videregående kalkulus. Men her kan vi få et nøyaktig uttrykk for restleddet ved å sette funksjonen i telleren til å være restleddet fra en Taylor-tilnærming med orden \\(k-1\\), altså\n\\[\nR_k(x) = f(x) - \\left( f(x_0) + f'(x_0) (x-x_0) + \\ldots + \\frac{1}{k!} f^{(k-1)}(x_0) (x-x_0)^{k-1} \\right)\n\\]\nog funksjonen i nevneren\n\\[\ng(x)=g_k(x)=\\frac{(x-x_0)^k}{k!}.\n\\]\nCauchys middelverdisetning gir\n\\[\n\\frac{R_k(x)}{g_k(x)} =\n\\frac{R_k(x)-R_k(x_0)}{g_k(x)-g_k(x_0)} = \\frac{R'_k(a_1)}{g_k'(a_1)} = \\frac{R'_k(a_1)-R'_k(x_0)}{g'_k(a_1)-g'_k(x_0)}\n= \\frac{R''_k(a_2)}{g''_k(a_2)},\n\\]\nhvor vi har brukt at både \\(R_k(x_0), R'_k(x_0), \\ldots\\), \\(g_k(x_0),g'_k(x_0),\\ldots\\) og alle første \\(k-1\\) deriverte er null ved \\(x_0\\). Ved å fortsette slik \\(k\\) ganger, får vi\n\\[\n\\frac{R_k(x)}{g_k(x)} = \\frac{R_k^{(k)}(a_k)}{g^{(k)}_k(a_k)} = f^{(k)}(a_k),\n\\]\nsiden \\(R_k^{(k)}(x) = f^{(k)}(x)\\) og \\(g^{(k)}_k(x) = 1\\), slik at\n\\[\nR_k(x) = \\frac{(x-x_0)^k}{k!} f^{(k)}(a_k),\n\\]\nhvor \\(a_k\\) er en konstant mellom \\(x_0\\) og \\(x\\). Siden vi ikke er interessert i de andre konstantene \\(a_1, a_2, \\ldots\\) døper vi om \\(a_k\\) til \\(a\\). Vi har bevist vårt feilestimat.\n\n\n2. Feilestimering\nRestleddet i en Taylor-tilnærming med \\(k-1\\) ledd er altså \\[\nR_k(x) = \\frac{(x-x_0)^k}{k!} f^{(k)}(a)\n\\]\nDet er oftest ikke mulig å finneut hva som \\(a\\) faktisk er. I praksis vil vi heller ofte si at\n\\[\nR_k(x) \\leq \\frac{(x-x_0)^k}{k!} \\max_{a\\in(x_0,x)} f^{(k)}(a)\n\\]\n\n\nEksempel 2\nFinn et estimat av feilen i tilnærmingen\n\\[\n\\log(2) \\approx (2-1) - \\frac{1}{2}(2-1)^2 = \\frac{1}{2}.\n\\]\nLa \\(f(x)=\\log(x)\\). Vi har \\(f'(x)=\\frac{1}{x}=x^{-1}\\), og \\(f''(x)=-x^{-2}\\), slik at \\(f^{(3)}(x)=2x^{-3}\\). Altså\n\\[\nR_3(x) = \\frac{(x-1)^3}{3!} 2 a^{-3} = \\frac{(x-1)^3}{3a^3},\\quad 1&lt;a&lt;x.\n\\]\nFor \\(x=2\\), har vi\n\\[\nR_3(2) = \\frac{1}{3a^3}, \\quad 1&lt;a&lt;2.\n\\]\nVi vet ikke hva \\(a\\) er, men vi kan iallfall forsikre oss at feilen ikke blir større en maksverdien til \\(R_3(2)\\), betraktet som en funksjon av \\(a\\). Siden den er en avtagende funksjon for positive \\(a\\) er den definitivt mindre enn verdien med \\(a=1\\), altså\n\\[\nR_3(2) &lt; \\frac{1}{3}.\n\\]\nDen ekte feilen er\n\\[\nR_3(2) = \\log(2) - \\frac{1}{2} = 0.193.\n\\]\n\n\nEksempel 3\nHvor mange ledd trenger vi i Taylor-tilnærmingen til \\(\\sin(x)\\) rundt \\(x_0=0\\) for å finne \\(\\sin(1)\\) til 4 desimalers nøyaktighet?\nFørst finner vi Taylor utviklingen. Vi har \\(f'(x)=-\\cos(x)\\), \\(f''(x)=-\\sin(x)\\), \\(f^{(3)}(x)=-\\cos(x)\\), osv. i samme mønster. Vi kan uttrykke det matematisk med:\n\\[\nf^{(2k)}=(-1)^k \\sin(x), \\quad f^{(2k+1)}=(-1)^k \\cos(x),\n\\]\nsiden alle partall er av formen \\(n=2k\\), og alle oddetall \\(n=2k+1\\).\nSiden \\(\\sin(0)=0\\) og \\(\\cos(0)=1\\) får vi\n\\[\nf^{(2k)}(0)=0, \\quad f^{(2k+1)}(0)=(-1)^k\n\\]\nAlle partallsledd forsvinner altså, og Taylor-tilnærmingen blir\n\\[\n\\sin(x) = x - \\frac{1}{3!}x^3 + \\ldots + \\frac{1}{(2k-1)!} (-1)^{k-1} x^{2k-1} + R_{2k+1},\n\\]\nhvor uttrykket for restleddet er\n\\[\nR_{2k+1} = \\frac{1}{(2k+1)!} (-1)^k \\cos(a), \\quad 0\\leq a\\leq 1\n\\]\nSiden \\(0&lt;\\cos(a)\\leq 1\\) på intervallet vet vi\n\\[\n|R_{2k+1}| \\leq \\frac{1}{(2k+1)!}\n\\]\nFor at \\(|R_{2k+1}|&lt;10^{-4}\\) trenger vi dermed at \\((2k+1)! &gt; 10^4\\). Det får vi først når \\(k=4\\). Dvs vi får 4 desimalers nøyaktighet med\n\\[\n\\sin(1) \\approx 1 - \\frac{1}{3!} + \\frac{1}{5!} - \\frac{1}{7!} = 1 - \\frac{1}{3} + \\frac{1}{120} - \\frac{1}{5040} =\n0.8415\n\\]\n\n\n\n\n\nTaylors teorem med flere variabler\nVi fokuserer på funksjoner av type \\(\\mathbb{R}^n\\rightarrow\\mathbb{R}\\), og ser spesielt på \\(n=2\\).\nVi skal altså finne tilnærminger\n\\[\nf(x,y) = P_k(x,y) + R_{k+1}(x,y)\n\\]\nhvor \\(P_k(x,y)\\) er et polynom. Vi fokuserer bare på \\(k=1\\) (linearisering) og \\(k=2\\).\n\nLinearisering\nEt lineært polynom i 2 variabler tar formen\n\\[\nP_1(x,y) = ax + by + c = c + \\vec{a}\\cdot\\vec{x}, \\quad\n\\vec{a}=\\begin{pmatrix}\na \\\\\nb\n\\end{pmatrix}^T, \\quad\n\\vec{x}=\\begin{pmatrix}\nx \\\\\ny\n\\end{pmatrix},\n\\]\nFørste ordens Taylor-tilnærming, \\(\\textit{lineariseringen}\\), er\n\\[\nf(\\vec{x}) = f(\\vec{x}_0) + \\Big[\\frac{Df}{D\\vec{x}}(\\vec{x_0})\\Big](\\vec{x}-\\vec{x_0}) + R_2\n\\]\n\n\nEksempel 1\nLa\n\\[\nf(x,y) = \\sqrt{9-x^2-y^2} = (9-x^2-y^2)^{\\frac{1}{2}}\n\\]\nFinn\n\nlinearisering rundt \\((0,0)\\)\nlinearisering rundt \\((2,2)\\)\n\nI vårt eksempel er \\[\n\\frac{\\partial f}{\\partial x} = -x (9-x^2-y^2)^{-\\frac{1}{2}}, \\quad\n\\frac{\\partial f}{\\partial y} = -y (9-x^2-y^2)^{-\\frac{1}{2}}\n\\]\n\nVi finner en lineær tilnærming rundt \\(x=0,y=0\\). Der er \\(\\frac{Df}{D\\vec{x}} = \\vec{0}\\), slik at tilnærmingen blir konstant:\n\n\\[\nf(x,y) \\approx f(0,0) = 3\n\\]\n\nTar vi tilnærmingen rundt \\(x=y=2\\) istedet, får vi \\(\\frac{Df}{D\\vec{x}} = (-2, -2)\\), og\n\n\\[\nf(x,y) \\approx f(2,2) + (-2, -2)\\begin{pmatrix}\nx-2 \\\\\ny-2\n\\end{pmatrix}\n= 1 + -2(x-2) + -2(y-2) = 9 - 2x - 2y\n\\]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nAndreordens\nEt andregradens polynom i to dimensjoner tar formen\n\\[\nP_2(x,y) = \\vec{x}^T A \\vec{x} + \\vec{b}\\cdot\\vec{x} + c,\n\\]\nfor en gitt matrise \\(A\\), vektor \\(b\\) og skalar \\(c\\).\nAndreordens Taylor-utvikling gir\n\\[\nf(\\vec{x}) = f(\\vec{x}_0) + \\Big[\\frac{Df}{D\\vec{x}}(\\vec{x}_0)\\Big](\\vec{x}-\\vec{x_0}) + \\frac{1}{2}(\\vec{x}-\\vec{x}_0)^T \\Big[\\frac{D^2f}{D\\vec{x}^2}(\\vec{x_0})\\Big] (\\vec{x}-\\vec{x_0})  + R_3,\n\\]\nder \\(\\frac{Df}{D\\vec{x}}\\) er en vektor, og \\(\\frac{D^2f}{D\\vec{x}^2}\\) er en matrise (Hessematrisen).\n\n\nEksempel 2\nVi fortsetter med samme funksjon \\(f(x,y) = \\sqrt{9-x^2-y^2}\\) som i eksempel 1.\nVi har \\[\n\\begin{align}\n\\frac{\\partial^2 f}{\\partial x^2} &= -(9-x^2-y^2)^{-\\frac{1}{2}} - x^2 (9-x^2-y^2)^{-\\frac{3}{2}} \\\\\n\\frac{\\partial^2 f}{\\partial x\\partial y} &= -xy (9-x^2-y^2)^{-\\frac{3}{2}} \\\\\n\\frac{\\partial^2 f}{\\partial y^2} &= -(9-x^2-y^2)^{-\\frac{1}{2}} - y^2 (9-x^2-y^2)^{-\\frac{3}{2}}\n\\end{align}\n\\]\nDa er \\[\n\\frac{Df}{D\\vec{x}}(0,0) = \\begin{pmatrix}\n-9^{\\frac{-1}{2}} & 0 \\\\\n0  & -9^{\\frac{-1}{2}}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-\\frac{1}{3} & 0 \\\\\n0 & -\\frac{1}{3}\n\\end{pmatrix}\n\\]\nAndreordens tilnærmingen rundt \\(x=0, y=0\\) blir da\n\\[\nf(x,y) \\approx f(0,0) + (x,y)\\begin{pmatrix}\n-\\frac{1}{3} & 0 \\\\\n0 & -\\frac{1}{3}\n\\end{pmatrix}\n\\begin{pmatrix}\nx \\\\\ny\n\\end{pmatrix}\n= 3 - \\frac{1}{3}x^2 - \\frac{1}{3}y^2\n\\]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nKritiske punkter (kommentar)\nNår ser vi hvorfor kritiske punkter er slik som vi har beskrevet tidligere! I et kritisk punkt er \\(\\frac{Df}{D\\vec{x}} = \\vec{0}\\), slik at\n\\[\nf(\\vec{x}) = f(\\vec{x}_0) + \\frac{1}{2}(\\vec{x}-\\vec{x}_0)^T \\Big[\\frac{D^2f}{D\\vec{x}^2}(\\vec{x_0})\\Big] (\\vec{x}-\\vec{x_0})  + R_3,\n\\] funksjonen kan attså tilnærmes av en av de tre variantene fra forrige uke.\n\n\nRestledd og nøyaktighet\n[ikke pensum]\nLa oss skrive den generelle formen slik:\n\\[\nf(\\vec{x})=f(\\vec{x_0}) + \\Big[\\frac{Df}{D\\vec{x}}(\\vec{x}_0)\\Big](\\vec{x}-\\vec{x}_0) + \\ldots\n+ \\frac{1}{n!}\\Big[\\frac{D^n f}{D\\vec{x}^n}(\\vec{x}_0)\\Big] (\\vec{x}-\\vec{x}_0,\\ldots,\\vec{x}-\\vec{x}_0)\n+ \\frac{1}{(n+1)!}\\Big[\\frac{D^{n+1} f}{D\\vec{x}^{n+1}}(\\vec{a})\\Big] (\\vec{x}-\\vec{x}_0,\\ldots,\\vec{x}-\\vec{x}_0)\n\\]\nHer er \\(\\vec{a}\\) en vektor som er minst like nær \\(\\vec{x}_0\\) som \\(\\vec{x}\\).\nVi forstår den deriverte \\(D^n f (x;h_1,\\ldots,h_n)\\) som en funksjon som tar en posisjon \\(\\vec{x}\\) og \\(n\\) retninger \\(h_1,\\ldots,h_n\\). I Taylors teorem er hver retning \\(\\vec{x}-\\vec{x}_0\\).\n\n\nEksempel 3: restledd i lineærisering\nLa oss se ta en titt på lineærisering av funksjonen \\(f(x,y) = \\sqrt{9-x^2-y^2}\\) rundt origoen. Restleddet er altså\n\\[\nR_2 = (x,y)\n\\begin{pmatrix}\n-(9-a^2-b^2)^{\\frac{-1}{2}} - a^2 (9-a^2-b^2)^{\\frac{-3}{2}} & -ab (9-a^2-b^2)^{\\frac{-3}{2}} \\\\\n-ab (9-a^2-b^2)^{\\frac{-3}{2}} & -(9-a^2-b^2)^{\\frac{-1}{2}} - b^2 (9-a^2-b^2)^{\\frac{-3}{2}}\n\\end{pmatrix}\n\\begin{pmatrix}\nx \\\\\ny\n\\end{pmatrix}\n\\]\nIkke spesielt pent!",
    "crumbs": [
      "Fellesmodul",
      "Del 4 – Taylors teorem"
    ]
  },
  {
    "objectID": "pages/page4.html",
    "href": "pages/page4.html",
    "title": "Del 3 – Kritiske punkt og optimering",
    "section": "",
    "text": "3.1 Kritiske punkt3.2 Kritiske punkt (fortsettelse)3.3 Optimering3.4 Med tre variabler\n\n\n\nKritiske punkt og optimering\nVi husker at et kritisk punkt er der hvor den deriverte \\(\\frac{df}{dx}=0\\). Det kan brukes til å finne lokale maksimum og minimum til en funksjon (optimering). Vi får vite om vi har en maks eller min ut fra fortegnet til den andrederiverte (hvis den eksisterer).\nFormålet med dette kapitlet er å vise hva som skjer for funksjoner av to variable. Det blir nok ganske likt situasjonen med en variabel, bare at det finnes flere typer kritiske punkt, og fortegnet til den andre deriverte må erstattes med fortegnet til egenverdiene til den andrederiverte (som er den symmetriske Hessematrisen).\n\nKritiske punkt I. Her definerer vi et kritisk punkt, og viser hvordan man kan bruke den andrederiverte til å finne ut hva slags kritisk punkt vi har, men bare når den andrederiverte er en diagonal matrise.\nKritiske punkt II. Hva gjør vi om den andrederiverte ikke er en diagonal matrise?\n\nMed diagonalisering (mest intuitiv)\nMed andrederivertstesten (mest lettvint)\n\nOptimering og kritiske punkt. Vi viser hvordan man kan finne maks/min av en funksjon innenfor et område ved å finne kritiske punkter og deretter undersøke langs randene.\nMed tre variabler. Hva gjør vi med funksjoner av tre variable?\n\nLæringsmål\n\nForstå intuitivt betydningen av kritiske punkt.\nIdentifisere kritiske punkt for funksjoner av to eller tre variable.\nKlassifisere kritiske punkt for funksjoner av to eller tre variable med den andrederiverte (gjennom diagonalisering og andrederivertstesten).\nForstå at den andrederiverte kan være null eller at den ikke eksisterer.\nBenytte kritiske punkt og partiellderivasjon til optimering av funksjoner av to variable.\n\n\nKritiske punkter\nEt punkt er et kritisk punkt når den deriverte \\(\\frac{\\partial f}{\\partial \\vec{n}} = 0\\) for alle retninger \\(\\vec{n}\\). I praksis holder det å sjekke at \\(\\frac{\\partial f}{\\partial x} = 0\\) og \\(\\frac{\\partial f}{\\partial y} = 0\\). (Hvorfor? Tenk på lineær algebra!).\nFra kalkulus av en variabel vet vi at i (lokale) maksimum- og minimumspunkt så er den deriverte lik null. Det samme gjelder for funksjoner av flere variable. I tillegg har vi i flere dimensjoner sadelpunkter. Sadelpunkter er punkter som ikke er lokale maksimum eller minimum.\n\n\nEksempel 1\nFunksjonen \\(f(x,y) = \\sin(x) + \\cos(y)\\) er avbildet under. Da har vi at de partiellderiverte er \\[\n\\frac{\\partial f }{\\partial x}(x,y)=\\cos(x),\\qquad \\frac{\\partial f }{\\partial y}(x,y)=-\\sin(y).\n\\] For å finne de kritiske punktene må vi finne ut hvor \\[\n\\cos(x)=0,\\quad \\text{og}\\quad-\\sin(y)=0.\n\\] Dette gir oss at \\(x=\\frac{\\pi}{2}+\\pi n\\) og \\(y=\\pi m\\) er kritiske punkt for alle heltall \\(n\\) og \\(m\\). Med andre ord er \\((\\frac{\\pi}{2}+\\pi n, \\pi m)\\) de kritiske punktene til funksjonen. Om vi plotter funksjonen kan vi se hvilken type kritisk punkt vi har. Klarer du å bestemme når vi har et maksimum- og når vi har et minimumspunkt ved å se på grafen? Hva med sadelpunkter?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nKritiske punkt I: Når den andrederiverte er en diagonal matrise\nEksempel 1 viser alle de tre typene kritiske punkt vi er interesert i; (lokale) maksimum, (lokale) minimum og sadelpunkter. Det er enda ikke helt klart hvordan vi kan vite hva slags kritisk punkt vi har uten å plotte funksjonen. Det er dette vi skal se på nå.\nHusk at den andrederiverte av en funksjon \\(f(x,y)\\) er hessematrisen \\[\nH =\n\\left(\\begin{array}{cc}\n     \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x\n     \\partial y}\\\\\n     \\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial\n     y^2}\n   \\end{array}\\right)\n\\] Når \\(H\\) er en diagonalmatrise i det kritiske punktet \\((x_0,y_0)\\), dvs. \\(\\frac{\\partial^2 f }{\\partial x\\partial y}(x_0,y_0) = 0\\), finnes det en lett regel for å klassifisere det kritiske punktet. I dette tilfellet er det nok å skjekke fortegnet til \\(A=\\frac{\\partial^2 f}{\\partial x^2}(x_0,y_0)\\) og \\(B=\\frac{\\partial^2 f}{\\partial y^2}(x_0,y_0)\\). Det er hovedsakelig tre muligheter:\n\nNår \\(A\\) og \\(B\\) er negative er det kritiske punktet et lokalt maksimum.\nNår \\(A\\) og \\(B\\) er positive er det kritiske punktet et lokalt minimum.\nVi får et sadelpunkt når \\(AB\\) er negativ, det vil si at \\(A\\) og \\(B\\) har forskjellig fortegn.\n\nDen siste muligheten er at \\(A\\) eller \\(B\\) er lik null (eller ikke finnes), og da kan vi ikke bruke andrederivertstesten for å klasifisere det kritiske punktet. Å klasifisere det kritiske punktet i dette tilfellet er ikke pensum.\n\n\n1. Maks\nLokalt ser et maksimum sånn ut. Funksjonen \\(f(x,y) = -x^2 - y^2\\) er avbildet under. Alternativt kunne vi ha skrevet den\n\\[\nf(\\vec{x}) =\n\\begin{pmatrix}\nx &y\n\\end{pmatrix}\n\\begin{pmatrix}\n-1 & 0 \\\\\n0 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\nx \\\\\ny\n\\end{pmatrix}\n\\] I dette tilfellet har vi at \\[\\frac{\\partial f }{\\partial x}(x,y)=-2x,\\qquad \\frac{\\partial f }{\\partial y}(x,y)=-2y.\\] Dermed er punktet \\((0,0)\\) det eneste kritiske punktet.\nI dette tilfellet er \\[\\frac{\\partial^2 f }{\\partial x\\partial y}=0\\] og \\[\\frac{\\partial^2 f}{\\partial x^2}=\\frac{\\partial^2 f}{\\partial y^2}=-2&lt;0.\\] Dermed er vi i tilfelle 1. over, og funksjonen har et lokalt makspunkt i punktet \\((0,0)\\). En grundigere forklaring kommer i neste kapittel, når vi ser på Taylors teorem.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n2. Min\nFunksjonen \\(f(x,y) =  x^2 + y^2\\) er avbildet under og vi kan se at den har et minimum i punktet \\((0,0)\\). Alternativt kunne vi ha skrevet funksjonen som\n\\[\nf(\\vec{x}) =\n\\begin{pmatrix}\nx &y\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx \\\\\ny\n\\end{pmatrix}.\n\\] I dette tilfellet har vi at \\[\\frac{\\partial f }{\\partial x}=2x,\\qquad \\frac{\\partial f }{\\partial y}=2y.\\] Dermed er punktet \\((0,0)\\) det eneste kritiske punktet.\nI dette tilfellet er \\[\\frac{\\partial^2 f }{\\partial x\\partial y}=0\\] og \\[\\frac{\\partial^2 f}{\\partial x^2}=\\frac{\\partial^2 f}{\\partial y^2}=2&gt;0.\\] Dermed er vi i tilfelle 2. hvor \\(A\\) og \\(B\\) er positiv, og funksjonen har et lokalt minimum i punktet \\((0,0)\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n3. Sadelpunkt\nFunksjonen \\(f(x,y) = x^2 - y^2\\) er avbildet under, og har et sadelpunkt i punktet \\((0,0)\\). Legg merke til at om vi går i en rett linje langs \\(x\\)-aksen ser det ut som et minimum, mens langs \\(y\\)-aksen ser det ut som et maksimum. Dette gjør at funksjonen ser ut som en “sadel” rundt punktet og derav navnet “sadelpunkt”. Alternativt kunne vi ha skrevet funksjonen som\n\\[\nf(\\vec{x}) =\n\\begin{pmatrix}\nx &y\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\nx \\\\\ny\n\\end{pmatrix}.\n\\] I dette tilfellet har vi at \\[\\frac{\\partial f }{\\partial x}=2x,\\qquad \\frac{\\partial f }{\\partial y}=-2y.\\] Dermed er igjen punktet \\((0,0)\\) det eneste kritiske punktet.\nI dette tilfellet er \\[\\frac{\\partial^2 f }{\\partial x\\partial y}=0\\] og \\[\\frac{\\partial^2 f}{\\partial x^2}=2&gt;0, \\qquad \\frac{\\partial^2 f}{\\partial y^2}=-2&lt;0.\\] Dermed er vi i tilfelle 3. hvor \\(A\\) og \\(B\\) har forskjellig fortegn, og funksjonen har et sadelpunkt i \\((0,0)\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nAndre deriverte lik null/eksisterer ikke\nSom nevnt, gir ikke andrederivertstesten noe informasjon når \\(A\\) eller \\(B\\) er null (eller ikke eksisterer).\nVi kan ikke utelukke at vi fortsatt har et kritisk punkt av en av de tre typene over. Et eksempel er funksjonen \\(f(x,y) = x^4 + y^4\\), som har et minimum i origo. Her er også hessematrisen lik null.\nDerimot vil funksjonen \\(g(x,y) = x^3 + y^3\\), ha et kritisk punkt i origo som er verken et lokalt maksimum eller minimum. Vi har plottet funksjonen under.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nEksempel 1 (fortsettelse)\nLa oss gå tilbake til eksempelet \\(f(x,y) = \\sin(x) + \\cos(y)\\).\nSom vi har vist er \\[\n\\frac{\\partial f}{\\partial x}(x,y) = \\cos(x), \\quad \\frac{\\partial f}{\\partial y}(x,y) = -\\sin(y)\n\\]\nDa har vi kritiske punkter der \\(\\cos(x) = 0\\) og \\(-\\sin(y)=0\\). Med andre ord har vi uendelig mange løsninger \\((x,y)\\), hvor \\[x=\n\\ldots,-\\frac{\\pi}{2},\\frac{\\pi}{2}, \\frac{3\\pi}{2}, \\ldots = \\frac{m\\pi + 1}{2}\\] og \\[y=\\ldots,-\\pi, 0, \\pi, 2\\pi, \\ldots = n\\pi.\\]\nDe andre deriverte er \\[\n\\frac{\\partial^2 f}{\\partial x^2}(x,y) = -\\sin(x), \\quad\n\\frac{\\partial^2 f}{\\partial y^2}(x,y) = -\\cos(y), \\quad\n\\frac{\\partial^2 f}{\\partial x\\partial y}(x,y) = 0,\n\\]\nslik at Hessematrisa er diagonal. Vi beregner\n\\[\n\\frac{\\partial^2 f}{\\partial x^2}\\left(\\frac{m\\pi + 1}{2},n\\pi \\right) = -\\sin\\left(\\frac{m\\pi + 1}{2}\\right) = (-1)^m, \\quad\n\\frac{\\partial^2 f}{\\partial y^2}\\left(\\frac{m\\pi + 1}{2},n\\pi\\right) = -\\cos(n\\pi)=(-1)^{n+1}\n\\]\nDermed får kritiske punkter av alle de tre typene:\n\nMaks når \\(m\\) er odde og \\(n\\) er jevn\nMin når \\(m\\) er jevn og \\(n\\) er odde\nSadel om \\(m\\) og \\(n\\) er begge odde eller begge jevn.\n\nFor eksempel, har vi et makspunkt ved \\((x,y) = (\\frac{\\pi + 1}{2},0)\\), siden det tilsvarer \\(m=1\\) og \\(n=0\\).\n\n\n\n\n\nDiagonalisering og andrederivertstesten\nI 3_1 så vi kun på tilfellet når hessematrisen var diagonal. La oss se hvordan vi kan klassifisere de kritiske punktene når hessematrisen ikke er en diagonalmatrise.\n\nKritiske punkter 2: Når hessematrisen ikke er diagonal\n\n\nEksempel 1\nLa oss se på funksjonen \\[\nf(x,y) = xy - x^2 - y^2 + x + 4y.\n\\] For å finne de kritiske punktene regner vi ut de partiellderivere \\[\n\\frac{\\partial f}{\\partial x}(x,y) = y - 2x + 1, \\quad\n\\frac{\\partial f}{\\partial y}(x,y) = x - 2y + 4.\n\\]\nDet viser seg at vi har et kritisk punkt, nemlig punktet hvor \\(x = 2\\) og \\(y=3\\). Dette kan vi bekrefte ved å løse ligningssystemet \\[\n\\begin{align}\ny - 2x + 1 &= 0 \\\\\nx - 2y + 4 &= 0.\n\\end{align}\n\\]\nFor å finne hessematrisen regner vi ut \\[\n\\frac{\\partial^2 f}{\\partial x^2} = -2, \\quad\n\\frac{\\partial^2 f}{\\partial y^2} = -2, \\quad\n\\frac{\\partial^2 f}{\\partial x\\partial y} = 1\n\\]\nHessematrisen er altså ikke diagonal og lik \\[\nH = \\begin{pmatrix}\n-2 & 1 \\\\\n1 & -2\n\\end{pmatrix}\n\\]\nHva gjør vi da? Vi har to muligheter:\n\nDiagonalisering av \\(H\\) for å deretter bruke testen vi lærte i 3_1.\nAndrederivertstesten som vi lærer i dette notatet.\n\nMulighet 2 er, som vi skal se, mer lettvint, mens mulighet 1 gir bedre forståelse av hva som foregår.\n\n\n1. Diagonalisering\nMatrisen \\(H\\) er symmetrisk, og kan dermed diagonaliseres. Det vil si (husk matte 1!) det finnes en basis \\(P=[\\vec{p}_1,\\vec{p}_2]\\) slik at \\[\nH = PDP^{-1},\n\\] hvor \\(D\\) er en diagonalmatrise. Basisen vi velger har ingen inflytelse på om et punkt er et lokalt maksimum, lokalt minimum eller sadelpunkt. Derfor kan vi se bort fra \\(P\\), og det kritiske punktet kan karakterises ut fra fortegnet til elementene i \\(D\\) (altså egenverdiene), akkurat som i 3_1.\n\n\nEksempel 1 (fortsettelse)\nHusk at diagonalelementene til \\(D\\) er egenverdiene \\(\\lambda_1, \\lambda_2\\), mens \\(P=[\\vec{p}_1,\\vec{p}_2]\\) dannes av de tilsvarende egenvektorene.\nI dette tilfelle er (beregnet i kodefeltet under)\n\\[\nD = \\begin{pmatrix}\n-1 & 0 \\\\\n0 & -3\n\\end{pmatrix},\\quad\nP = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}\n1 & -1 \\\\\n1 & 1\n\\end{pmatrix}\n\\]\nBegge egenverdier har negativt fortegn. Derfor har vi et makspunkt.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nVi kan bekrefte at dette stemmer i punktet \\((2,3)\\) ved å plotte funksjonen i \\([1,3]\\times[2,4]\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n2. Andrederivertstesten\nDet finnes en del oppskrifter som lar deg hoppe over å diagonalisere (men som egentlig utledes fra dette). Disse blir ofte kalt andrederivert-testen.\nLa \\(\\Delta=|H|\\) være determinanten til hessematrisen i det kritiske punktet \\((x_0,y_0)\\). Da har vi at 1. hvis \\(\\Delta&gt;0\\) og \\(\\frac{\\partial^2 f}{\\partial x^2}(x_0,y_0)&lt; 0\\), så har vi et lokalt maksimumspunkt. 2. hvis \\(\\Delta&gt;0\\) og \\(\\frac{\\partial^2 f}{\\partial x^2}(x_0,y_0)&gt;0\\), så har vi et lokalt minimumspunkt. 3. hvis \\(\\Delta&lt;0\\) så har vi et sadelpunkt.\nOm \\(\\Delta=0\\) betyr det at en av egenverdiene til \\(H\\) må være null. Dermed ender vi opp i tilfellet hvor vi ikke får noe informasjon.\n\n\nEksempel 1 (fortsettelse)\nLa oss igjen se på funksjonen \\[\nf(x,y) = xy - x^2 - y^2 + x + 4y.\n\\] Tar vi determinanten til hessematrisen får vi \\[\n|H| = \\left|\n\\begin{pmatrix}\n-2 & 1 \\\\\n1 & -2\n\\end{pmatrix}\n\\right|\n= -2\\cdot (-2) - 1\\cdot 1 = 3\n\\]\nSiden \\(\\Delta = 3 &gt; 0\\), og \\(\\frac{\\partial^2 f}{\\partial x^2}(2,3)= \\frac{\\partial^2 f}{\\partial y^2}(2,3) = -2 &lt; 0\\), har vi et maksimumspunkt.\n\n\nEksempel 2\nVi ser på funksjonen\n\\[\nf(x,y) = \\sin(xy).\n\\]\nVi har\n\\[\n\\frac{\\partial f}{\\partial x}(x,y) = y\\cos(xy), \\quad\n\\frac{\\partial f}{\\partial y}(x,y) = x\\cos(xy).\n\\] For å finne de kritiske punktene løser vi ligningsettet \\(y\\cos(xy)=0\\) og \\(x\\cos(xy)=0\\). Da får vi at \\(f\\) har kritiske punkter i \\((0,0)\\) og når \\(xy=\\frac{\\pi}{2}+\\pi n\\) hvor \\(n\\) er et heltall.\nFor å klassifisere de kritiske punktene regner vi ut \\[\n\\frac{\\partial^2 f}{\\partial x^2}(x,y) = -y^2 \\sin(xy), \\quad\n\\frac{\\partial^2 f}{\\partial x\\partial y}(x,y) = \\cos(xy) - xy\\cos(xy), \\quad\n\\frac{\\partial^2 f}{\\partial y^2}(x,y) = -x^2 \\sin(xy).\n\\]\nVi ønsker å undersøke det kritisk punkt ved \\((0,0)\\). Da er hessematrisen lik\n\\[\nH = \\begin{pmatrix}\n0 & 1 \\\\\n1 & 0\n\\end{pmatrix}.\n\\]\nDeterminanten \\(\\Delta = 0^2 - 1^2 = -1\\). Siden determinanten er negativ har vi et sadelpunkt i origo. Dette bekreftes i bildet under.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nHvorfor fungerer andrederivertstesten? [Ikke pensum]\nDet er på grunn av at determinanten til en diagonaliserbar matrise er lik produktet av egenverdiene. Hvis fortegnet til disse er forskjellige, får vi at \\(\\Delta\\) er negativ, ellers blir \\(\\Delta\\) positiv. Dette vil si at metode 1 og 2 i dette notatet er ekvivalente.\n\n\n\n\n\nOptimering\nHer skal vi se på hvordan vi kan bruke kritiske punkter til å finne globale maksimum og minimum på lukkede domener i \\(\\mathbb{R}^2\\).\nVi begynner med en repetisjon av tilsvarende optimering i 1D, før vi gå videre til 2D.\n\n1. Funksjoner av en variabel (repetisjon)\nNøkkelen her er følgende setning: en kontinuerlig funksjon \\(f:[a,b]\\rightarrow \\mathbb{R}\\) har en maksimumsverdi og en minimumsverdi.\nEr funksjonen i tillegg deriverbar kan vi finner maksimumsverdien og minimumsverdien som følger:\n\nFinn alle kritiske punkter \\(x\\).\nMaksimums- og minimumspunkter er enten et kritisk punkt eller på randen (\\(a\\) eller \\(b\\)).\nBeregne \\(f(x)\\) for alle kritiske punkter, i tillegg til \\(f(a)\\) og \\(f(b)\\).\nVelg den minste/største verdien.\n\n\n\nEksempel 1\nLa oss se på \\(f(x) = x^3 - x^2 - x + 1\\), \\(-2\\leq x \\leq 2\\).\nHva er maksimums- og minimumsverdien?\nVi beregner\n\\[\nf'(x) = 3x^2 - 2x - 1 = (3x+1)(x-1)\n\\]\nVi har altså kritiske punkter ved \\(x=-\\frac{1}{3}\\) og \\(x=1\\). Den andre deriverte er\n\\[\nf''(x) = 6x - 2,\n\\]\nslik at \\(f''(1)=4\\) og \\(f''(-\\frac{1}{3})=-4\\). Dermed er \\(x=1\\) et lokalt minimumspunkt og \\(x=-1\\) et lokalt maksimumspunkt.\nVi må sjekke også endepunktene/randpunktene \\(x=-2\\) og \\(x=2\\). Om vi skriver ned funksjonsverdien i disse fire punktene får vi\n\n\\(f(-2)=(-2)^3 - (-2)^2 - (-2) + 1 = -8 - 4 + 2 + 1 = -9\\)\n\\(f(-\\frac{1}{3}) = (-\\frac{1}{3})^3 - (-\\frac{1}{3})^2 - (-\\frac{1}{3}) + 1 = 1.185\\)\n\\(f(1) = 1^3 - 1^2 - 1 + 1 = 0\\)\n\\(f(2) = 2^3 - 2^2 - 2 + 1 = 8 - 4 - 2 + 1 = 3\\).\n\nSiden den minste verdien er i punktet \\(x=-2\\) er \\(-9\\) minimumsverdien til \\(f\\), mens den største er i punktet \\(x=2\\), som gir oss maksimumsverdi \\(3\\). La oss dobbeltsjekke ved å plotte funksjonen.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nEksempel 2: Funksjon uten maks/min\nLivet er ikke alltid like lett! La \\(f:(0,1)\\rightarrow \\mathbb{R}\\) være definert som\n\\[\nf(x) = \\frac{1}{x}.\n\\]\nDenne funksjonen har ikke noen maksimumsverdi, siden \\(f(x)\\) går mot uendelig når \\(x\\rightarrow 0\\). Vi kan heller ikke sette inn \\(x=0\\), siden dette punktet ikke er i definisjonsområdet til funksjonen.\nDette strider ikke mot vår tidligere setning, som kun gjaldt for deriverbare funksjoner hvor definisjonsområdet \\([a,b]\\) er et lukket intervall, dvs. endepunktene er med.\n\n\n2. Funksjoner av to variabler\nEn funksjon \\(f: D \\subset \\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) har garantert en maksimumsverdi/minimumsverdi hvis \\(D\\) er kompakt. Kompakt betyr at randen er med i \\(D\\), samtidig som \\(D\\) ikke inneholder punkter uendelig langt vekke fra origo. Vi skal ikke fokusere på hva det vil si å være kompakt, men et eksempel er et lukket rektangel \\([a,b]\\times[c,d]\\).\nI så fall kan vi finne maks/min punkter ved å følge samme fremgangsmåte som for en variabel, med en forskjell: randen består ikke lenger av to punkter \\((a,b)\\), men er generelt mer komplisert.\nFor eksempel, for \\([a,b]\\times [c,d]\\) består randen av 4 linjer: 1. \\(x=a, c\\leq y \\leq d\\) 2. \\(x=b, c\\leq y \\leq d\\) 3. \\(a\\leq x\\leq b, y=c\\) 4. \\(a\\leq x\\leq b, y=d\\)\nVi må altså sjekke hvor \\(f(x)\\) er største/minste på de fire linjene over. Hver linje gir oss et optimeringsproblem i en variabel, f.eks. på linje 1 må vi finne maksimums-/minimumsverdien av\n\\[\ng(y) = f(a,y), \\quad c\\leq y\\leq d.\n\\]\nMaksimums- og minimumsverdien av \\(f\\) kan også befinne seg i hjørnene av \\([a,b]\\times[c,d]\\). Det vil si at vi må finne den største/minste verdien av de kritiske punktene, på punktene på linjene 1.-4., og de fire hjørnepunktene \\((a,c), (a,d), (b,c), (b,d)\\).\n\n\nEksempel 3\nLa oss se på \\[\nf(x,y) = x^2 - 2xy + 2y^2 - 2y + 1,\n\\] definert på \\(0\\leq x\\leq 3, 0\\leq y\\leq 2\\).\n\nFørste finner vi de partiellderiverte:\n\n\\[\n\\frac{\\partial f}{\\partial x}(x,y) = 2x - 2y, \\quad\n\\frac{\\partial f}{\\partial y}(x,y) = -2x + 4y - 2\n\\]\n\nDeretter finner vi de kritiske punktene ved å løse: \\[\n\\begin{pmatrix}\n2 & -2 \\\\\n-2 & 4\n\\end{pmatrix}\n\\begin{pmatrix}\nx \\\\\ny\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\\n2\n\\end{pmatrix}.\n\\] Her får vi en entydig løsning \\(x=1, y=1\\).\nVi får fire endimensjonale optimeringsproblemer langs randen:\n\n\\(x=0, 0\\leq y\\leq 2\\). Her er \\[\ng_1(y)=f(0,y) = 2y^2 - 2y + 1,\n\\] Funksjonen \\(g_1\\) har kritiske punkter når \\(g_1'(y)=4y-2=0\\), altså \\(y=0.5\\). Dermed er en ny maksimums/minimumspunkt kandidat \\(x=0, y=0.5\\)\n\\(x=3, 0\\leq y\\leq 2\\). Her er \\[\ng_2(y)=f(3,y) = 2y^2 -8y + 10\n\\] Som har kritiske punkter når \\(g_2'(y)=4y-8=0\\), altså \\(y=2\\). En ny kandidat er da \\(x=0, y=2\\)\n\\(y=0, 0\\leq x\\leq 3\\). Her er \\[\ng_3(x)=f(x,0) = x^2 + 1\n\\] Som har kritiske punkter når \\(g_3'(x)=2x=0\\), alstå \\(x=0\\). En ny kandidat er da \\(x=y=0\\)\n\\(y=2, 0\\leq x\\leq 3\\). Her er \\[\ng_4(x)=f(x,2) = x^2 - 4x + 5\n\\] Som har kritiske punkter når \\(g_4'(x)=2x-4=0\\), altså \\(x=2\\). En ny kandidat er da \\(x=2, y=2\\)\n\nVi må også sjekke hjørnene: \\((0,0), (0,2),(3,0),\\) og \\((3,2)\\).\n\nVi har:\n\\[\n\\begin{align}\nf(1,1) &= 1^2 - 2\\cdot 1\\cdot 1 + 2\\cdot 1^2 - 2\\cdot 1 + 1 = 0 \\\\\nf(0,0.5) &= 0^2 - 2\\cdot 0\\cdot 0.5 + 2\\cdot 0.5^2 - 2\\cdot 0.5 + 1 = 0.5 \\\\\nf(2,2) &= 2^2 - 2\\cdot 2\\cdot 2 + 2\\cdot 2^2 - 2\\cdot 2 + 1 = 1 \\\\\nf(0,0) &= 0^2 - 2\\cdot 0\\cdot 0 + 2\\cdot 0.5^0 - 2\\cdot 0 + 1 = 1 \\\\\nf(0,2) &= 0^2 - 2\\cdot 0\\cdot 2 + 2\\cdot 2^2 - 2\\cdot 2 + 1 = 5 \\\\\nf(3,0) &= 3^2 - 2\\cdot 3\\cdot 0 + 2\\cdot 0^2 - 2\\cdot 0 + 1 = 10 \\\\\nf(3,2) &= 3^2 - 2\\cdot 3\\cdot 2 + 2\\cdot 2^2 - 2\\cdot 2 + 1 = 2\n\\end{align}\n\\]\nDen største er \\(f(3,0)=10\\), dvs. \\(f\\) har maksimumsverdien 10 i punktet \\(x=3, y=0\\). Minimumsverdien er \\(f(1,1)=0\\) i minimumspunktet \\(x=1, y=1\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nEksempel 4\nHva med funksjonen \\(f(x,y) = \\sqrt{2x^2 + y^2}\\), for \\(x^2 + y^2 \\leq 4\\)? Siden randen \\(x^2 + y^2 = 4\\) er med, og den ikke inneholder punkter uendelig langt vekke fra origo, har \\(f\\) en minimumsverdi og en maksimumsverdi.\nDet eneste kritiske punktet til funksjoen \\(f\\) er i origo som er et lokalt minimumspunkt med minimumsverdi \\(0\\) (se oppgaven under). Så vi har ingen maksimumskandidater inni sirkelen, og disse må derfor ligge på randen. Randen til \\(x^2 + y^2 \\leq 4\\) er sirkelen \\(x^2 + y^2 = 4\\). Regner vi ut \\(f\\) langs randen får vi\n\\[\nf(x,y) = \\sqrt{2x^2 + y^2} = \\sqrt{x^2 + (x^2 + y^2)} = \\sqrt{x^2 + 4}\n\\]\nMaksimumspunktene er altså hvor \\(x\\) (og dermed \\(x^2\\)) er størst i området \\(x^2 + y^2 \\leq 4\\), nemlig ved \\(x=\\pm 2\\). Tilsvarende \\(y\\)-verdi langs randen er \\(0\\), siden dette er eneste måten å oppfylle \\(x^2 + y^2 = 4\\) når \\(x=\\pm 2\\). Siden vi ikke har noen lokale maksimumspunkter inni sirkelen, vil \\((\\pm 2, 0)\\) være maksimumspunktene som gir maksimumsverdi \\(f(\\pm 2, 0)=\\sqrt{8}\\).\nMinimumspunktet lang randen er når \\(x=0\\), som gir oss \\(f(0,\\pm 2)=2\\). Dermed er minimumspunktet \\((0,0)\\) og minimumsverdien \\(0\\). #### Oppgave: Vis at det eneste kritiske punktet til funksjonen \\(f\\) er i origo, og vis at dette er et lokalt minimumspunkt.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\nFunksjoner av tre variabler\nEn god del av det vi har diskutert gjelder også funksjoner med 3 variabler. Derimot er det vanskeligere å visualisere kritiske punkter for funksjoner med tre variabler.\nSamme optimeringsmetode som 3_3 fungerer, men blir fort mer tidkrevende. For eksempel om vi optimerer over en kube, må løse 2-dimensjonale optimeringsproblemer på 6 overflater og 8 kanter, i tillegg til å sjekke de kritiske punktene.\nVi nøyer oss med et eksempel av kritiske punkter for en funksjon med tre variabler.\n\nEksempel 1.\nLa\n\\[\nf(x,y,z) = (x+y)(xy + z).\n\\]\nDa er\n\\[\n\\frac{\\partial f}{\\partial x}(x,y,z) = (xy + z) + y(x+y), \\quad\n\\frac{\\partial f}{\\partial y}(x,y,z) = (xy + z) + x(x+y), \\quad\n\\frac{\\partial f}{\\partial z}(x,y,z) = (x + y).\n\\]\nVi må sette de partiellderiverte lik null, for å finne de kritiske punktene. Dette medfører at \\(x+y=0\\), som vi kan sette inn i uttrykket for \\(\\frac{\\partial f}{\\partial x}\\) og \\(\\frac{\\partial f}{\\partial y}\\). Da får vi ligningssystemet\n\\[\nx + y = 0, \\quad xy + z = 0.\n\\]\nVi setter \\(y=-x\\) inn i den andre ligningen, og får\n\\[\nz = x^2.\n\\]\nAlle punkter på formen \\((x, -x, x^2)\\) er altså kritiske punkter.\nVi finner de andreordens partiellderiverte:\n\\[\n\\frac{\\partial^2 f}{\\partial x^2}(x,y,z) = 2y, \\quad\n\\frac{\\partial^2 f}{\\partial x\\partial y}(x,y,z) = 2x, \\quad\n\\frac{\\partial^2 f}{\\partial y^2}(x,y,z) = 2x\n\\] \\[\n\\frac{\\partial^2 f}{\\partial x\\partial z}(x,y,z) = 1, \\quad\n\\frac{\\partial^2 f}{\\partial y\\partial z}(x,y,z) = 1, \\quad\n\\frac{\\partial^2 f}{\\partial z^2}(x,y,z) = 0.\n\\]\nHessematrisen er altså:\n\\[\nH =\n\\begin{pmatrix}\n2y & 2x & 1 \\\\\n2x & 2x & 1 \\\\\n1 & 1 & 0\n\\end{pmatrix}\n\\]\nEt kritisk punkt hvor hessematrisen kun har negative egenverdier er et maksimumspunkt, om alle egenverdiene er positive et minimumspunkt, og hvis hessematrisen har både positive og negative egenverdier et sadelpunkt (så lenge at \\(H\\) eksisterer). Merk at i tilfellene hvor hessematrisen har minst en null egenverdi og resten er ikke positiv (eller ikke negativ) kan vi ikke konkludere hvilken type kritisk punkt vi har utifra hessematrisen.\nLa oss se på det kritiske punktet i origoen. Her er\n\\[\nH = \\begin{pmatrix}\n0 & 0 & 1 \\\\\n0 & 0 & 1 \\\\\n1 & 1 & 0\n\\end{pmatrix}\n\\]\nVi ser under at egenverdiene er \\(-\\sqrt{2},0,\\sqrt{2}\\). Siden \\(H\\) har både positive og negative egenverdier har vi et sadelpunkt.\nLegg merke til at begrepet sadelpunkt kan ikke lenger tolkes som et sadel!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Fellesmodul",
      "Del 3 – Kritiske punkt og optimering"
    ]
  },
  {
    "objectID": "pages/page2.html",
    "href": "pages/page2.html",
    "title": "Del 1 – Funksjoner av flere variabler",
    "section": "",
    "text": "1.1 Definisjoner1.2 Visualisering1.3 Eksempler\n\n\n\nFunksjoner genereltFunksjoner av flere variable\n\n\nHer kommer en påminnelse av diverse begrep rundt funksjoner som vi lærte i matematikk 1. Mer informasjon om begrepene kan du finne ved å lese her.\nEn funksjon \\(f\\) som går fra mengden \\(X\\) til mengden \\(Y\\) blir ofte skrevet som \\(f:X\\rightarrow Y\\). En funksjon kan tenkes på som en regel, en måling eller en algoritme som sender hvert element \\(x\\in X\\) til et unikt annet element \\(f(x)\\in Y\\). De ulike mengdene forbundet med funksjonen \\(f\\) har følgende navn:\n\nDefinisjonsmengde (domene, kilde, input): \\(X\\)\nVerdiområde: \\(Y\\)\nVerdimengde (kodomene, bilde, output): Undermengden \\(f(X)\\) av \\(Y\\), som består av alle elementene \\(f(x)\\) som vi kan lage ved å anvende \\(f\\) på elementene \\(x\\in X\\).\n\nVi har sett i matematikk 1 at funksjoner kan være injektive, surjektive og bijektive. Hvis du trenger en påminnelse, så kan du gå tilbake til fagstoffet i matematikk 1. Du kan også lese mer her. En kort oppsummering av disse egenskapene er: - Injektiv (en-til-en): Alle elementer i verdimengden \\(f(X)\\) treffes av bare et element \\(x\\in X\\). - Surjektiv: Alle elementer i verdiområdet \\(Y\\) kan treffes. - Bijektiv: Både injektiv og surjektiv.\n\nEksempel 1 - endelige mengderEksempel 2 - lineære transformasjonerEksempel 3 - løsninger til ordinære differensialligninger\n\n\nFor eksempel, om domenet er mengden \\(X = \\{1,2,3\\}\\) og verdiområdet er \\(Y=\\{4,5,6\\}\\) kan vi lage funksjonene \\(f:X\\to Y\\) og \\(g:X\\to Y\\) som følger:\n\n\\(f(1)=4\\), \\(f(2)=5\\), \\(f(3)=4\\)\n\\(g(x)=x+3\\)\n\nFunksjonen \\(g\\) er bijektiv, da alle elementene i \\(Y\\) er truffet av nøyaktig ett element i \\(X\\).\nFunksjoner definert på endelige mengder er kanskje mest relevant for IMAx2024.\n\n\nI matematikk 1 så vi på lineære transformasjoner (se lenkene over for en påminnelse).\nTa for eksempel matrisetransformasjonen \\(f(\\vec{x}) = A\\vec{x}\\), hvor \\[\nA = \\begin{pmatrix}\n1 & 2 & 0 \\\\\n-2 & 1 & -1\n\\end{pmatrix}.\n\\]\n\nOppgave:\nFor funksjonen \\(f\\) over, hva er\n\ndefinisjonsmengden?\nverdiområde?\nverdimengde?\n\nEr transformasjonen \\(f\\) gitt over\n\ninjektiv?\nsurjektiv?\nbijektiv?\n\n\n\n\nVi ser på differensialligningen:\n\\[\nx^2 \\frac{d^2 y}{dx^2} + x\\frac{dy}{dx} + (x^2-1) y = 0, \\quad y(0) = 0,\\, y'(0) = 1.\n\\]\nVi vet (ved å skrive om til en ligning av 1. orden) at den har en entydig løsning \\(y:\\mathbb{R}\\rightarrow\\mathbb{R}\\). Men vi vet ikke om verdimengden er hele \\(\\mathbb{R}\\). Heller ikke om det finnes en enkel formel for \\(y(x)\\) (det vil si om \\(y\\) er en elementær funksjon).\nI matematikk 1 har vi også lært om løsninger til differensiallligninger med vektorer, f. eks.\n\\[\n\\frac{d}{dt} \\vec{x}(t) = \\begin{pmatrix}\n0 & 1 \\\\\n-1 & 0\n\\end{pmatrix}\n\\vec{x}(t), \\quad\n\\vec{x}(0) = \\begin{pmatrix}\n1 \\\\ 0\n\\end{pmatrix}.\n\\] Her er løsningen \\(\\vec{x}\\) en funksjon fra \\(\\mathbb{R}\\) til \\(\\mathbb{R}^2\\).\n\nOppgave:\n\nPlott funksjonen \\(y\\) gitt som løsningen på problemet over. Hva tror du defininsjonsmengden, verdiområdet og verdimengden er?\nGjør det samme med løsningen til \\[\ny' = y^2 + 1, \\quad y(0) = 0.\n\\]\n\n\n\n\n\n\n\nEn funksjon av flere variable, eller flervariabelfunksjon, i matematikk 2 er en funksjon \\(f:\\mathbb{R}^n\\rightarrow \\mathbb{R}\\). Vi sier da også at \\(f\\) er en funksjon av \\(n\\) variable.\nFunksjoner på den mer generelle formen \\(f:\\mathbb{R}^n\\rightarrow \\mathbb{R}^m\\) er tema i matematikk 3. Når \\(m&gt;1\\) så sier vi også at funksjonen er vektorvaluert.\nEn funksjon \\(f:\\mathbb{R}^n\\rightarrow \\mathbb{R}\\) kan forstås som en funksjon fra vektorer (eller punkter) til skalarer (dvs. tall). Slike funksjoner kalles derfor også skalare funksjoner.\nFor eksempel så kan en funksjon av to variable forstås som en funksjon på vektorer i planet. Vi kan skrive enten \\(f:\\mathbb{R} \\times\n\\mathbb{R} \\rightarrow \\mathbb{R}\\) eller \\(\\mathbb{R}^2\\rightarrow\\mathbb{R}\\) for tovariabelfunksjoner.\nVi kommer til å ha spesielt fokus på funksjoner av to variable. La oss se på noen eksempler på slike funksjoner.\n\nEksempel 4Eksempel 5Eksempel 6\n\n\nLa \\(f:\\mathbb{R}^2\\to \\mathbb{R}\\) være gitt ved formelen \\[f(x,y)=x-y.\\] Dette er et eksempel på en funksjon av to variable. Her kan vi sette inn et punkt \\((x,y)\\), som for eksempel \\(x=1\\) og \\(y=2\\), og få ut verdien \\[f(1,2)=1-2=-1.\\] Dette er også et eksempel på en lineær transformasjon, da vi kan skrive funksjonen på matriseformen \\[f(x,y)=\\begin{pmatrix}\n1 & -1\n\\end{pmatrix}\\begin{pmatrix}\nx\\\\  y\n\\end{pmatrix}.\\]\n\n\nEt annet eksempel er funksjonen \\(g:\\mathbb{R}^2\\to \\mathbb{R}\\), som er definert som \\[g(x,y)=\\cos(x+y).\\] Hvis vi bruker verdiene \\(x=5\\pi\\) og \\(y=3\\pi\\), så får vi\n\\[g(5\\pi, 3\\pi)=\\cos(5\\pi+3\\pi)=\\cos(8\\pi)=1.\\]\nEn annen måte å si det på er at om vi setter inn punktet \\((5\\pi, 3\\pi)\\) i \\(f\\), så får vi ut verdien \\(g(5\\pi, 3\\pi)=1.\\)\n\n\nEt siste eksempel er funksjonen \\(h:\\mathbb{R}^3\\to \\mathbb{R}\\) definert som \\[h(x,y,z)=x^2-y^2+z.\\] Om vi setter inn punktet \\((1,2,3)\\), så får vi verdien \\[h(1,2,3)=1^2-2^2+3=1-4+3=0.\\]\n\nOppgave:\n\nHva får du om du setter inn punktet \\((4,1)\\) i funksjonen \\(f\\)?\nHvor mange variable har funksjonene \\(g\\) og \\(h\\) i eksemplene over?\nHva får du om du setter inn punktet \\((2,3,4)\\) i funksjonen \\(h\\)?\n\n\n\n\n\n\n\n\n\n\nEn funksjon \\(\\mathbb{R}\\rightarrow\\mathbb{R}\\) kan visualiseres ved å tegne et bilde av grafen til funksjonen. Hvor lett det er å visualisere funksjoner \\(\\mathbb{R}^m\\rightarrow\\mathbb{R}^n\\) er avhengig av dimensjonene \\(n\\) og \\(m\\). Det er ikke lett å visualisere vektorer med dimensjon 4 eller større.\nI dette notatet kommer vi til å se på visualiseringe av følgende funksjoner:\n\nKurveplott for \\(f:\\mathbb{R} \\rightarrow\\mathbb{R}^2\\) og \\(f:\\mathbb{R} \\rightarrow\\mathbb{R}^3\\)\nVisualiseringer for skalare funksjoner av to variabler \\(f:\\mathbb{R }^2\\rightarrow\\mathbb{R}\\). Vi skal spesifikt se på:\n\nOverflateplott\nNivåkurveplott\nVarmeplott\n\n\n\nKurveplottVisualiseringer for skalare funksjoner av to variabler\n\n\n\nKodeskisse for plottingKurveplott i planetKurveplott i rommet\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nLa oss si at vi ønsker å plotte banen til kurven\n\\[f(t)=\\left(\\frac{t + t^3}{1 + t^4}, \\frac{t - t^3}{1 + t^4}\\right).\\]\nLegg merke til at når \\(t\\to \\infty\\) kommer denne kurven til å nærme seg origo, siden\n\\[\\lim_{t\\to \\infty}\\frac{t + t^3}{1 + t^4}=\\lim_{t\\to \\infty}\\frac{t - t^3}{1 + t^4}=0.\\]\nVi kan kun plotte kurven på endelige intervaller. Om vi velger å plotte kurven begrenset til intervallet \\([-12, 12]\\), kan vi bruke følgende kode:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nLegg merke til at kurveplottet ikke gir oss informasjon om hvor partikelen befinner seg ved forskjellige tidspunkt. Det vil si at vi ikke vet hvor \\(f(1)\\) bare ved å se på plottet. Likevel gir kurveplott oss en nyttig visualisering om hvordan partikelen beveger seg.\n\n\nPå en lignende måte kan vi visualisere en kurve i rommet. I koden under plotter vi kurven\n\\[g(t)=\\left(\\left(\\frac{t^2}{4 \\pi^2  } + 1\\right)\\sin(t),\\left(\\frac{t^2}{4 \\pi^2} + 1\\right)\\cos(t),\\frac{t}{2\\pi} \\right)\\]\nfor verdier i intervallet \\([-4\\pi, 4\\pi]\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSom vi ser, kommer partikelen til å bevege seg langs en slags spiral. Legg også merke til den første linjen med kode i cellen over. Her gir vi beskjed om at vi ønsker å plotte i 3D.\n\nOppgave:\nBruk kodefeltet under til å tegne kurven\n\\[f(t)=\\left(\\cos(t), \\sin(t)\\right)\\]\nfor intervallet \\([0,\\pi]\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nDet finnes flere muligheter for visualisering av en funksjon \\(f:\\mathbb{R}^2\\to \\mathbb{R}\\). Den mest direkte analogien til grafen av en envariabelfunksjon er det som kalles overflateplott. I tillegg til dette skal vi også lære om nivåkurveplott og varmeplott.\n\nOverflateplottNivåkurveplottVarmeplott\n\n\nLa oss si at vi ønsker å visualisere funksjonen\n\\[f(x,y)=(1 - x/2 + x^5 + y^3) \\exp(-x^2 - y^2).\\]\nIgjen må vi velge hvor vi ønsker å tegne grafen. I koden under tegner vi den på rektangelet \\([-3,3]\\times [-4, 4]\\). Det vil si at vi tegner overflaten \\(x\\) verdier i intervallet \\([-3,3]\\) og \\(y\\) verdier i intervallet \\([-4, 4]\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nOm vi ønsker, kan vi lese av punktverdier fra overflateplottet. Da velger vi et punkt i planet på bunnen og trekker en linje rett oppover til det treffer overflaten. Vi kan for eksempel se at i punktet \\((3,2)\\) er funksjonen tilnærmet lik \\(0\\). Vi kan også se at den største verdien er omtrent \\(1\\) og ligger over punktet \\((0,0)\\).\nVi kan endre på hvordan grafen blir tegnet om vi ønsker. For eksempel så kan vi legge til farger som gjør overflateplottet lettere å forstå.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nEt alternativ til overflateplott er et såkalt nivåkurveplott (eng. contour plot). En nivåkurve er en kurve som består av alle punkter \\((x,y)\\) hvor funksjonen er lik en bestemt konstant verdi. For eksempel er nivåkurven til funksjonen\n\\[g(x,y)=x^2+y^2\\]\nfor verdien \\(1\\) kurven består av alle verdier \\((x,y)\\) slik at \\(g(x,y)=x^2+y^2=1\\). Vi vet fra før at ligningen \\(x^2+y^2=1\\) beskriver en sirkel sentrert i origo med radius 1. Om vi ønsker kan vi lage et plott som viser flere nivåkurver til denne funksjonen. Igjen må vi velge en rektangulær del av planet, og denne gangen har vi valgt \\([-1, 2]\\times [-2, 2]\\).\nLegg merke til at nivåkurvene er sirkler, selv om det ikke ser slik ut på bildet. Grunnen er at bildet bruker ulik steglengde i \\(x\\) og \\(y\\)-retning. Du kan få nivåkurver som ligner mer på sirkler hvis du velger et annet rektangel.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nOm vi går tilbake til overflateplottet til funksjonen\n\\[f(x,y)=(1 - x/2 + x^5 + y^3) \\exp(-x^2 - y^2)\\]\nser vi at overflateplottet ser litt ut som et fjell. Hvis vi har en funksjon som beskriver høyden over havet i et landskap, så vil høydekurvene på et topografisk kart være et nivåkurveplott. Dere kan se på et eksempel her.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nEt plott som ligner på nivåkurveplottet er et såkalt varmeplott. Her prøver vi å plotte alle nivåkurvene samtidig ved å bruke en fargegradient. Om vi følger en bestemt farge rundt på bilde så følger vi en nivåkurve til funksjonen. La oss se hvordan vi kan lage et varmeplott for funksjonen \\[f(x,y)=(1 - x/2 + x^5 + y^3) \\exp(-x^2 - y^2).\\]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nMotivasjonen til navnet varmeplott er som følger: Sett at vi har en stålplate som ligger horisontalt. Vi varmer opp platen på endene og lar \\(f(x,y)\\) beskrive temperaturen til et punkt \\((x,y)\\) på platen. Et varmeplott av funksjonen \\(f\\) gir oss i dette tilfellet informasjon om temperaturen til de ulike områdene på stålplaten.\n\nOppgave:\nLag et overflateplott og et varmeplott til funksjonen\n\\[g(x,y)=x^2+y^2.\\]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nHer følger noen programrelevante eksempler av funksjoner.\nDet er sikkert mye her som oppleves som helt ukjent. Det er greit! Du vil lære mer om dette ute på studieprogrammene. Men som du kan se, så blit flervariabelfunksjoner brukt svært mye.\nDet er ofte slik at viktige fysiske størrelser som kraft, moment, tetthet og temperatur varierer over en konstruksjon, og er derfor funksjoner av posisjon. Kraft og moment er vektorer, og vi kan se på et kraftfelt som \\(F: \\mathbb{R}^3\\rightarrow\\mathbb{R}^3\\) som en funksjon av posisjon. Det samme er tilfellet for moment. Siden tetthet er skalar kan vi representere tettheten \\(\\rho\\) til en posisjon i rommet som en funksjon \\(\\rho:\\mathbb{R}^3\\rightarrow\\mathbb{R}\\). Det samme gjelder for eksempel temperatur.\nMen vi ser også på systemer i planet (eller langs linjer), hvor vi pga. symmetrier (eller lignende) ignorerer avhengighet i en retning. Da kan vi se på kraft som en funksjon \\(F:\\mathbb{R}^2\\rightarrow\\mathbb{R}^2\\). Dette er en forenkling som vi ofte gjør på jordas overflate, når vi ignorerer høyden, slik som for vind og havstrømmer.\nHva med spenning? Den er generelt avhengig av både posisjon \\(\\vec{x}\\) og et tverrsnitt (det er nok å oppgi bare en normalvektor \\(\\vec{n}\\)), og er en vektor selv. Den er også en funksjon \\(\\tau(\\vec{x},\\vec{n}):\\mathbb{R}^3 \\times \\mathbb{R}^3 \\rightarrow \\mathbb{R}^3\\), men det er ikke hele historien. Hvis vi tar en vilkårlig \\(\\vec{x}\\) og holder den konstant er funksjonen \\(\\tau(\\vec{x},\\vec{n}):\\mathbb{R}^3\\rightarrow\\mathbb{R}\\) en lineær transformasjon, og kan dermed skrives som en matrise! Enda bedre viser det seg (se Bell, Konstruksjonsmekanikk, II.9) at matrisen er symmetrisk! Dermed er det trolig best å tenke på spenning som en funksjon \\(\\mathbb{R}^3\\rightarrow Y\\), hvor \\(Y\\) er rommet av \\(3\\times 3\\) symmetriske matriser.\nTøyning er en annen funksjon av samme type (se Bell, Konstruksjonsmekanikk, II.10).\n\nElektrofagInformatikkKjemi og materialteknologiLogistikkGeomatikkTermodynamikkFysikk\n\n\nGrunnleggende elektrolære bruker vektorfelt, dvs. funksjoner på formen \\(\\mathbb{R}^3\\rightarrow\\mathbb{R}^3\\). Eksempler er elektriske og magnetiske felt. Vi har også strømtetthet, elektrisk forskyvning/polarisering og magnetisering, som også er vektorfelt. Ladingstetthet og elektrisk potensial er eksempler på skalare funksjoner \\(\\mathbb{R}^3\\rightarrow\\mathbb{R}\\).\nI mange tilfeller er funksjonene over også avhengig av tid, dvs. de er på formen \\(\\mathbb{R}^3 \\times \\mathbb{R}\\rightarrow \\mathbb{R}^3\\), (eller \\(\\mathbb{R}^4\\rightarrow \\mathbb{R}^3\\)).\nFor elektriske kretser så er vi mest opptatt av forhold mellom forskjellige størrelser (variabler), f.eks. Ohms lov, hvor \\(V=IR\\) beskriver en funksjon \\(V:\\mathbb{R}^2 \\rightarrow \\mathbb{R}\\). I analyse av vekselstrøm er det vanlig å utvide til komplekse tall, slik at vi har funksjoner \\(V:\\mathbb{C}^2 \\rightarrow \\mathbb{C}\\).\nDet kan også hende at vi trenger flere variable, som for eksempel temperatur. Hvis motstand er avhengig av temperatur, så får vi \\(V=R(T)I\\), slik at \\(V:\\mathbb{R}^3\\rightarrow\\mathbb{R}\\).\n\n\nFunksjoner er overalt i informatikken. Vi kommer til å se flere eksempler i diskret matematikk, men også i bildebehandling og datavitenskap/maskinlæring er det mange interessante eksempler.\nVi nevner at et bilde kan sees som en funksjon \\(b:\\mathbb{R}^2 \\mapsto \\mathbb{R}^3\\), hvor vi oppgi en fargeverdi (i f.eks. tredimensjonalt RGB-rom) til hvert punkt \\((x,y)\\) i bildet. I praksis vil vi bare lagre verdier på endelig mange punkter \\((x_i, y_i)\\), dvs. vi bruker en diskretisering av funksjonen.\n\n\nMaterialteknologi bruker mange av de samme funksjonene som mekanikk (spenning, tøyning osv.) og termodynamikk (temperatur, entropi, varmekapasitet osv.).\nI tillegg er konsentrasjonen av et stoff viktig. Det er typisk et reelt tall. Hvis konsentrasjonen varierer med posisjon, så vil vi ha en funksjon \\(\\mathbb{R}^3 \\rightarrow \\mathbb{R}\\).\nEt mer komplisert eksempel av en egenskapet til et material er refleksjonsfaktor (https://snl.no/refleksjonsfaktor). I en enkel tilnærming er det bare et skalartall, men mer generelt kan det være avhengig av både inn- og utvinklene, og muligens også bølgelengden (https://en.wikipedia.org/wiki/Bidirectional_reflectance_distribution_function).\n\n\nVi har skalare funksjoner \\(f(t)\\) når vi sporer lagerføring av en vare på en fabrikk gjennom tiden.\nMen hva skjer hvis vi vil spore varer over alle steder i systemet, inkludert på veier hvor posisjonen endrer seg kontinuerlig? Da ender vi opp med funksjoner av type \\(X \\times \\mathbb{R} \\rightarrow \\mathbb{R}\\). Om \\(X\\) er en enkel veg kommer vi til å ende opp med en funksjon \\(\\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R}\\).\n\n\nGeoiden er et klassisk eksempel på en funksjon \\(U: \\mathbb{R}^2 \\rightarrow \\mathbb{R}\\). Variablene til denne funksjonen er breddegrader og lengdegrader.\nFunksjonen \\(U\\) kommer fra en mer generell funksjon \\(\\mathbb{R}^3 \\rightarrow\\mathbb{R}\\) som er tyngdekraftspotensialet gitt som en funksjon av posisjon i rom. Geoiden er en nivåflate til denne funksjonen.\n\n\nTermodynamikk er en viktig anvendelse av temaene i dette notatet. Mange opplever termodynamikk som vanskelig, ofte på grunn av manglende forståelse av funksjoner av flere variable.\nTilstanden i et enkelt kompressibelt system er beskrevet av to uavhengig variable, typisk bruker vi to av disse tre: temperatur (\\(T\\)), trykk (\\(\\rho\\)) og volum (\\(V\\)). Da kan alle andre termodynamiske størrelser, som varme, entropi, entalpi osv. være gitt som funksjoner \\(\\mathbb{R}^2 \\rightarrow \\mathbb{R}\\).\nVisualisering har en lang historie i dette tilfelle. I læreboka “Engineering thermodynamics” ser man en overflate i rommet med trykket til vann som en funksjon av volum og temperatur. Den skotske fysikeren James Clerk Maxwell konstruerte til og med en fysisk modell av noen lignende i gips, se wikipedia for mer informasjon.\n\n\nEksemplene fra mekanikk og elektrofag er begge relevant i fysikk.\nI tillegg har vi fluidmekanikk, som har noen fellestrekk med begge. Her er (det tidsavhengige) hastighetsfeltet \\(u:\\mathbb{R}^3\\times\\mathbb{R}\\rightarrow\\mathbb{R}^3\\), samt trykket \\(p:\\mathbb{R}^3\\times\\mathbb{R}\\rightarrow\\mathbb{R}\\) viktig. Om systemet er konstant i tid blir de istedet henholdsvis \\(u:\\mathbb{R}^3\\rightarrow\\mathbb{R}^3\\) og \\(p:\\mathbb{R}^3\\rightarrow\\mathbb{R}\\).\nFor å finne posisjonen til en partikkel i strømmen, så må man løse differensialligningen\n\\[\n\\dot{x}(t) = u\\big(x(t),t\\big).\n\\]\nI motsetning til elektriske felt, finnes det ikke noen fysisk strømpotensial. Men noen forenklede modeller tillater en slik funksjon. Det er en stor hjelp når man skal trekke konklusjoner, men kan ofte føre til fysiske feil.\nVi har også spennings- og forflytningstensor, akkurat som i mekanikk.",
    "crumbs": [
      "Fellesmodul",
      "Del 1 – Funksjoner av flere variabler"
    ]
  },
  {
    "objectID": "pages/page7.html",
    "href": "pages/page7.html",
    "title": "02 - Numeriske metoder",
    "section": "",
    "text": "Fokuset i dette emnet er numeriske løsninger. Vi begynner derfor med en innføring i “hva og hvordan” angående numeriske løsninger, og diskuterer numeriske evalueringer av de partiellderiverte, som er grunnsteinen som alle våre metoder baserer seg på.\n\n\nUten en slags formel eller en algoritme, så trenger vi uendelig mye data til å beskrive en funksjon \\(\\mathbb{R}\\rightarrow\\mathbb{R}\\).\nLikevel kommer vi langt hvis funksjonen er deriverbar, og vi kjenner en endelig mengde med verdier \\((x_i, f(x_i))\\).\n\n\nTa en titt på koden under og hva som skjer når vi tegner grafen til \\(f(x)=\\sin(\\pi x)\\). For å lage grafen har vi altså ikke brukt noe kunnskap om funksjonen \\(\\sin(x)\\) utenom de \\(100\\) verdiene \\(\\sin(x_i)\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPå samme måte, hvis vi har en funksjon \\(\\mathbb{R}^2\\rightarrow\\mathbb{R}\\) av to variabler, er det ofte nok å ha en mengde med verdier \\(\\big(x_j, y_i, f(x_j, y_i)\\big)\\).\nHvorfor kommer \\(j\\) før \\(i\\)? Se diskusjon som kommer etter kodefeltet/plottet\nNår vi plotter funksjoner for eksempel, lager vi et rutenett av punkter \\((x_j, y_i)\\) og beregner funksjonsverdiene \\(f(x_j, y_i)\\). Ingen annen informasjon brukes:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nDu lurte kanskje på hvorfor vi brukte \\(f(x_j,y_i)\\), og ikke \\(f(x_i,y_j)\\)?\nNår vi skriver matriser kommer tradisjonelt radene først, dvs. tallet \\(A[i,j]\\) er i rad \\(i\\) og kolonne \\(j\\).\nMen når vi skal tegne grafen til en funksjon \\(f(x,y)\\), er det vanligere å la \\(x\\)-aksen gå horisontalt og \\(y\\)-aksen vertikalt. Det mest naturlige rutenettet har derfor \\(x\\)-retning tilsvarende kolonner, og \\(y\\) tilsvarende rader. Vi setter derfor opp\n\\[\nF[i,j] = f(x_j, y_i)\n\\]\nNumpy sin meshgrid-funksjon lager som default et rutenett \\(X[i,j], Y[i,j]\\) slik at \\(X[i,j]=x[j]\\) og \\(Y[i,j]=y[i]\\). Resultatet er at \\(F = F(X,Y)\\) gir arrayen over.\nDet er imidlertidig mulig å overstyre slik at man heller får \\(F[i,j]=f(x_i,y_j)\\) hvis man foretrekker det. Du kan prøve dette i kodefeltet under.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nHva med de partiellderiverte? Hvordan skal vi finne dem dersom vi bare vet noen funksjonsverdier \\(f(x_j, y_i)\\)?\n\n\nHusk fra ProgNumSikk(PNS) at hvis vi har en funksjon \\(f(x)\\) av én variabel, så finnes det tre naturlige måter å tilnærme den deriverte \\(f'(x)\\) på:\n\nForlengs differanser\n\n\\[\nf'(x) \\approx \\frac{f(x+h)-f(x)}{h}\n\\]\n\nBaklengs differanser\n\n\\[\nf'(x) \\approx \\frac{f(x)-f(x-h)}{h}\n\\]\n\nSentrale differanser\n\n\\[\nf'(x) \\approx \\frac{f(x+h)-f(x-h)}{2h}\n\\]\nDet er ingen hindring i å bruke disse formlene om vi kun vet funksjonsverdiene på enkelte punkter \\(f(x_i)\\). Det vi gjør er å la \\(h\\) være avstanden mellom tilstøtende punkter, dvs:\n\nForlengs differanser\n\n\\[\nf'(x_i) \\approx \\frac{f(x_{i+1})-f(x_i)}{h}\n\\]\n\nBaklengs differanser\n\n\\[\nf'(x_i) \\approx \\frac{f(x_i)-f(x_{i-1})}{h}\n\\]\n\nSentrale differanser \\[\nf'(x_i) \\approx \\frac{f(x_{i+1})-f(x_{i-1})}{2h}\n\\]\n\nI formlene har vi antatt at avstanden \\(h\\) mellom punktet \\(x_i\\) og nabopunktene, \\(x_{i-1}\\) og \\(x_{i+1}\\) er konstant, altså at \\(x_{i+1} = x_i + h\\) for alle \\(i\\).\nVi viser hvordan dette fungerer under.\n\n\n\nHvilken metode har vi tatt i bruk i koden under? Kan du endre koden til å kjøre en av de andre?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPartiellederivasjon fungerer som over, bare at vi nå har et rutenett med punkter \\((x_j,y_i)\\).\nLa \\(h\\) være avstanden mellom punktene i \\(x\\)-retning. Vi har da følgende mulige beregninger av \\(f_x\\):\n\nForlengs differanser\n\n\\[\nf_x(x_j,y_i) \\approx \\frac{f(x_{j+1},y_i)-f(x_j,y_i)}{h}\n\\]\n\nBaklengs differanser\n\n\\[\nf_x(x_j, y_i) \\approx \\frac{f(x_j,y_i)-f(x_{j-1},y_i)}{h}\n\\]\n\nSentrale differanser \\[\nf_x(x_j, y_i) \\approx \\frac{f(x_{j+1},y_i)-f(x_{j-1},y_i)}{2h}\n\\]\n\nLa nå \\(k\\) være avstanden mellom punktene i \\(y\\)-retning. For \\(f_y\\) får vi\n\nForlengs differanser\n\n\\[\nf_y(x_j,y_i) \\approx \\frac{f(x_j,y_{i+1})-f(x_j,y_i)}{k}\n\\]\n\nBaklengs differanser\n\n\\[\nf_y(x_j,y_i) \\approx \\frac{f(x_j,y_{i})-f(x_j,y_{i-1})}{k}\n\\]\n\nSentrale differanser\n\n\\[\nf_y(x_j,y_i) \\approx \\frac{f(x_j,y_{i+1})-f(x_j,y_{i-1})}{2k}\n\\]\nVi viser hvordan dette gjøres under.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "PDE",
      "02 - Numeriske metoder"
    ]
  },
  {
    "objectID": "pages/page7.html#innføring",
    "href": "pages/page7.html#innføring",
    "title": "02 - Numeriske metoder",
    "section": "",
    "text": "Uten en slags formel eller en algoritme, så trenger vi uendelig mye data til å beskrive en funksjon \\(\\mathbb{R}\\rightarrow\\mathbb{R}\\).\nLikevel kommer vi langt hvis funksjonen er deriverbar, og vi kjenner en endelig mengde med verdier \\((x_i, f(x_i))\\).\n\n\nTa en titt på koden under og hva som skjer når vi tegner grafen til \\(f(x)=\\sin(\\pi x)\\). For å lage grafen har vi altså ikke brukt noe kunnskap om funksjonen \\(\\sin(x)\\) utenom de \\(100\\) verdiene \\(\\sin(x_i)\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPå samme måte, hvis vi har en funksjon \\(\\mathbb{R}^2\\rightarrow\\mathbb{R}\\) av to variabler, er det ofte nok å ha en mengde med verdier \\(\\big(x_j, y_i, f(x_j, y_i)\\big)\\).\nHvorfor kommer \\(j\\) før \\(i\\)? Se diskusjon som kommer etter kodefeltet/plottet\nNår vi plotter funksjoner for eksempel, lager vi et rutenett av punkter \\((x_j, y_i)\\) og beregner funksjonsverdiene \\(f(x_j, y_i)\\). Ingen annen informasjon brukes:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nDu lurte kanskje på hvorfor vi brukte \\(f(x_j,y_i)\\), og ikke \\(f(x_i,y_j)\\)?\nNår vi skriver matriser kommer tradisjonelt radene først, dvs. tallet \\(A[i,j]\\) er i rad \\(i\\) og kolonne \\(j\\).\nMen når vi skal tegne grafen til en funksjon \\(f(x,y)\\), er det vanligere å la \\(x\\)-aksen gå horisontalt og \\(y\\)-aksen vertikalt. Det mest naturlige rutenettet har derfor \\(x\\)-retning tilsvarende kolonner, og \\(y\\) tilsvarende rader. Vi setter derfor opp\n\\[\nF[i,j] = f(x_j, y_i)\n\\]\nNumpy sin meshgrid-funksjon lager som default et rutenett \\(X[i,j], Y[i,j]\\) slik at \\(X[i,j]=x[j]\\) og \\(Y[i,j]=y[i]\\). Resultatet er at \\(F = F(X,Y)\\) gir arrayen over.\nDet er imidlertidig mulig å overstyre slik at man heller får \\(F[i,j]=f(x_i,y_j)\\) hvis man foretrekker det. Du kan prøve dette i kodefeltet under.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "PDE",
      "02 - Numeriske metoder"
    ]
  },
  {
    "objectID": "pages/page7.html#diskretisering-og-de-partiellderiverte",
    "href": "pages/page7.html#diskretisering-og-de-partiellderiverte",
    "title": "02 - Numeriske metoder",
    "section": "",
    "text": "Hva med de partiellderiverte? Hvordan skal vi finne dem dersom vi bare vet noen funksjonsverdier \\(f(x_j, y_i)\\)?\n\n\nHusk fra ProgNumSikk(PNS) at hvis vi har en funksjon \\(f(x)\\) av én variabel, så finnes det tre naturlige måter å tilnærme den deriverte \\(f'(x)\\) på:\n\nForlengs differanser\n\n\\[\nf'(x) \\approx \\frac{f(x+h)-f(x)}{h}\n\\]\n\nBaklengs differanser\n\n\\[\nf'(x) \\approx \\frac{f(x)-f(x-h)}{h}\n\\]\n\nSentrale differanser\n\n\\[\nf'(x) \\approx \\frac{f(x+h)-f(x-h)}{2h}\n\\]\nDet er ingen hindring i å bruke disse formlene om vi kun vet funksjonsverdiene på enkelte punkter \\(f(x_i)\\). Det vi gjør er å la \\(h\\) være avstanden mellom tilstøtende punkter, dvs:\n\nForlengs differanser\n\n\\[\nf'(x_i) \\approx \\frac{f(x_{i+1})-f(x_i)}{h}\n\\]\n\nBaklengs differanser\n\n\\[\nf'(x_i) \\approx \\frac{f(x_i)-f(x_{i-1})}{h}\n\\]\n\nSentrale differanser \\[\nf'(x_i) \\approx \\frac{f(x_{i+1})-f(x_{i-1})}{2h}\n\\]\n\nI formlene har vi antatt at avstanden \\(h\\) mellom punktet \\(x_i\\) og nabopunktene, \\(x_{i-1}\\) og \\(x_{i+1}\\) er konstant, altså at \\(x_{i+1} = x_i + h\\) for alle \\(i\\).\nVi viser hvordan dette fungerer under.\n\n\n\nHvilken metode har vi tatt i bruk i koden under? Kan du endre koden til å kjøre en av de andre?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPartiellederivasjon fungerer som over, bare at vi nå har et rutenett med punkter \\((x_j,y_i)\\).\nLa \\(h\\) være avstanden mellom punktene i \\(x\\)-retning. Vi har da følgende mulige beregninger av \\(f_x\\):\n\nForlengs differanser\n\n\\[\nf_x(x_j,y_i) \\approx \\frac{f(x_{j+1},y_i)-f(x_j,y_i)}{h}\n\\]\n\nBaklengs differanser\n\n\\[\nf_x(x_j, y_i) \\approx \\frac{f(x_j,y_i)-f(x_{j-1},y_i)}{h}\n\\]\n\nSentrale differanser \\[\nf_x(x_j, y_i) \\approx \\frac{f(x_{j+1},y_i)-f(x_{j-1},y_i)}{2h}\n\\]\n\nLa nå \\(k\\) være avstanden mellom punktene i \\(y\\)-retning. For \\(f_y\\) får vi\n\nForlengs differanser\n\n\\[\nf_y(x_j,y_i) \\approx \\frac{f(x_j,y_{i+1})-f(x_j,y_i)}{k}\n\\]\n\nBaklengs differanser\n\n\\[\nf_y(x_j,y_i) \\approx \\frac{f(x_j,y_{i})-f(x_j,y_{i-1})}{k}\n\\]\n\nSentrale differanser\n\n\\[\nf_y(x_j,y_i) \\approx \\frac{f(x_j,y_{i+1})-f(x_j,y_{i-1})}{2k}\n\\]\nVi viser hvordan dette gjøres under.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "PDE",
      "02 - Numeriske metoder"
    ]
  },
  {
    "objectID": "TEMPLATE_LOCK.html",
    "href": "TEMPLATE_LOCK.html",
    "title": "Matematikk",
    "section": "",
    "text": "This project state is locked as the canonical template. Do not change layout, Pyodide loading, or Matplotlib setup. New modules must follow page2.qmd structure."
  }
]